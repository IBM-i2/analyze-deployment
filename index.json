{
  "configure_change_schema.html": {
    "href": "configure_change_schema.html",
    "title": "Updating the Schema",
    "keywords": "Updating the Schema In a deployment of i2 Analyze, the database structure is defined by a schema. To update the schema, you must update the database and then restart Liberty to load the updated schema. Solr requires no changes. Before you begin Ensure that you can connect to the deployment with the current schema, and submit data by using Analyst's Notebook Premium. Ensure that all of the Docker containers are running. Modifying the schema file In the distributed deployment example, the law-enforcement-schema.xml and law-enforcement-charting-schemes.xml files that represent the i2 Analyze schema are in the following directory: src/configuration/fragments/common/WEB-INF/classes . In Schema Designer, open the law-enforcement-schema.xml file from the classes directory, and make additive changes to the schema. After you modify the schema, save your changes and close Schema Designer. Copying the configuration In the distributed deployment example, the schema and charting schemes changes are completed on the Admin client and Liberty containers. To update the deployment with the changes, you must run toolkit tasks that interact with the Information Store database and the i2 Analyze application. The changed configuration must be copied to the admin_client and liberty containers. To copy, and overwrite, your configuration to the admin_client and liberty containers, run the following commands from the src directory: docker cp ./configuration/fragments/common/WEB-INF/classes admin_client:/opt/i2/i2analyze/toolkit/configuration/fragments/common/WEB-INF/ docker cp ./configuration/fragments/common/WEB-INF/classes liberty:/opt/i2/i2analyze/toolkit/configuration/fragments/common/WEB-INF/ The new configuration is now on the admin_client and liberty containers. When the directory is copied, the owner of the directory, and all files within it, is changed. The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following commands: docker exec -u root admin_client chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration docker exec -u root liberty chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration In a non-Docker environment, copy the law-enforcement-schema.xml and law-enforcement-charting-schemes.xml files to the same location on the Liberty server and ensure that the correct permissions are set. Stopping Liberty Before you update the Information Store with your modified schema, you must stop the application server. You can shut down the application by using the stopLiberty i2 Analyze toolkit task. In the distributed deployment example, run the following command in the liberty Docker container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty The output from the stopLiberty task is output directly to the console. Updating the database Update the database to conform to the updated schema. To update the schema, you can run the command from the Admin client or Liberty server. In the distributed deployment example, the Admin client is used to run the commands. To update the database to conform to the updated schema, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t updateInformationStoreSchema Updating the application You must update the application with the modified schema and charting scheme. Run the following commands on the Liberty server to deploy Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t deployLiberty Starting Liberty Start Liberty to load the updated schema and charting scheme: Run the following command to start Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. Testing the deployment To test that the schema is updated successfully, connect to the Information Store in Analyst's Notebook Premium. After you connect and log in, a message is displayed that informs you of a schema change. The changes that you made to the schema are visible in the palette, and available for you to use. What to do next After you modify the schema, you might need to modify the filters that users see during Quick Search and Visual Query operations. For more information, see Changing the search results filtering ."
  },
  "configure_change_security_schema.html": {
    "href": "configure_change_security_schema.html",
    "title": "Updating the Security Schema",
    "keywords": "Updating the Security Schema An i2 Analyze security schema defines the security dimension values that you can assign to items and records in the Information Store, and the security permissions that you assign to groups of users. Modify the security schema in the i2 Analyze configuration, then update the Information Store database to conform to the new security schema, then restart the application server. For more information about the i2 Analyze security schema, see Configuring the security schema . Before you begin Ensure that you can connect to the deployment with the current security schema, and submit data by using Analyst's Notebook Premium. Ensure that all of the Docker containers are running. Modifying the security schema file In the distributed deployment example, the example-dynamic-security-schema.xml file that represents the i2 Analyze security schema is in the following directory: src/configuration/fragments/common/WEB-INF/classes . In an XML editor, open the example-dynamic-security-schema.xml file from the classes directory. Modify the security schema. For more information about the changes that you can make to the security schema file, see Modifying the security schema . After you modify the file, save your changes. Copying the configuration In the distributed deployment example, the security schema changes are completed on the Admin client and Liberty containers. To update the deployment with the changes to the security schema, you must run toolkit tasks that interact with the Information Store database and the i2 Analyze application. The changed configuration must be copied to the admin_client and liberty containers. To copy, and overwrite, your configuration to the admin_client and liberty containers, run the following commands from the src directory: docker cp ./configuration/fragments/common/WEB-INF/classes admin_client:/opt/i2/i2analyze/toolkit/configuration/fragments/common/WEB-INF/ docker cp ./configuration/fragments/common/WEB-INF/classes liberty:/opt/i2/i2analyze/toolkit/configuration/fragments/common/WEB-INF/ The new configuration is now on the admin_client and liberty containers. When the directory is copied, the owner of the directory, and all files within it, is changed. The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following commands: docker exec -u root admin_client chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration docker exec -u root liberty chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration In a non-Docker environment, copy the example-dynamic-security-schema.xml file and any other modified files to the same location on the Liberty server and ensure that the correct permissions are set. Stopping Liberty Before you update the security schema, you must stop the application server. You can shut down the application by using the stopLiberty i2 Analyze toolkit task. In the distributed deployment example, run the following command in the liberty Docker container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty The console output from the stopLiberty task is output directly to the console. Clear the search index Some modifications to the security schema require you to clear the search index. For more information about the changes that require a reindex, see Modifying security dimensions and Modifying security permissions . If your changes require you to clear the search index, complete the rest of this section. To clear the search index, you can run the command from Admin client or Liberty server. In the distributed deployment example, the admin_client container is used to run the command. The admin_client container includes the i2 Analyze toolkit and a DB2 client installation. When you run the container, the remote Information Store database is cataloged. You can run commands that modify the database from this container. To clear the search index, run the following command: docker exec -u i2analyze -it admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t clearSearchIndex --all --hostname admin_client.eianet The search index is cleared, however any data that was in the database remains in the database. Updating the database Update the database to conform to the updated security schema. To update the database, you can run the command from the Liberty or Admin client server. In this distributed deployment example, the admin_client container is used to run the commands. The admin_client container includes the i2 Analyze toolkit and a DB2 client installation. When you run the container, the remote Information Store database is cataloged. You can run the commands that modify the database from this container. To update the database for the updated security schema, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t updateInformationStoreSecuritySchema Updating the application If you cleared the search index, you must also update the application. Run the following commands on the Liberty server to deploy Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t deployLiberty Starting Liberty Start Liberty to load the updated security schema. Run the following command to start Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. Testing the deployment To test that the security schema is updated successfully, you can change the user.registry.xml file to add a user to your new or updated groups and connect to the Information Store in Analyst's Notebook Premium. For more information about changing the user.registry.xml file, see Administering user access ."
  },
  "configure_liberty.html": {
    "href": "configure_liberty.html",
    "title": "Changing the memory allocation for Liberty",
    "keywords": "Changing the memory allocation for Liberty After you deploy your system, you might need to change the maximum Java virtual machine (JVM) memory allocation allowance for Liberty. Before you begin Ensure that you can connect to the deployment and submit data by using Analyst's Notebook Premium. Modifying the environment-advanced.properties file Modify the environment-advanced.properties in the src/configuration/environment/opal-server directory. To change the JVM memory allocation, modify the value of the was.heap.size property. For example, was.heap.size=4096 . Copying the configuration Your changes are required by the Liberty server, this is the server that applies the changes. The changed configuration must be copied to the liberty container. To use the docker cp function, the container must be running. To copy, and overwrite, your configuration to the liberty container, run the following command from the src directory: docker cp ./configuration/environment/opal-server/environment-advanced.properties liberty:/opt/i2/i2analyze/toolkit/configuration/environment/opal-server/ The new configuration is now on the liberty container. When the directory is copied, the owner of the directory, and all files within it, is changed. The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following command: docker exec -u root liberty chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration In a non-Docker environment, copy the modified environment-advanced.properties file to the same location on the Liberty server and ensure that the correct permissions are set. Updating the application Run the following commands on the Liberty server to stop, deploy, and start Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t deployLiberty docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty Testing the deployment To test the deployment, connect to i2 Analyze from Analyst's Notebook Premium."
  },
  "configure_search_facets.html": {
    "href": "configure_search_facets.html",
    "title": "Changing the search results filtering",
    "keywords": "Changing the search results filtering Modify the results filters in the i2 Analyze configuration, then redeploy the application server. Before you begin Ensure that you can connect to the deployment and submit data by using Analyst's Notebook Premium. To understand what search result filtering is, and why you might need to modify the filters, see Setting up search results filtering . Modifying the results configuration file The law-enforcement-schema-results-configuration.xml file in the distributed deployment example is in the following directory: src/configuration/fragments/common/WEB-INF/classes . In an XML editor, open the law-enforcement-schema-results-configuration.xml file from the classes directory, and modify the filters that are available for an item type. After you modify the file, save your changes. For more information about the results configuration file, see Understanding the results configuration file . Copying the configuration Your results configuration file changes are required by Liberty. The modified configuration must be copied to the Liberty server. In a Docker environment, use the docker cp command to copy, and overwrite, your configuration to the running liberty container. In the src directory, run the following command: docker cp ./configuration/fragments/common/WEB-INF/classes liberty:/opt/i2/i2analyze/toolkit/configuration/fragments/common/WEB-INF/ The new configuration is now on the liberty container. When the directory is copied, the owner of the directory, and all files within it, is changed. The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following command: docker exec -u root liberty chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration In a non-Docker environment, copy the modified results configuration file to the same location on the Liberty server and ensure that the correct permissions are set. Updating the application Run the following commands on the Liberty server to stop, deploy, and start Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t deployLiberty docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty Testing the deployment To test that the results configuration is updated successfully, connect to the Information Store in Analyst's Notebook Premium. After you connect and log in, complete a search that demonstrates if your results filter changes are applied."
  },
  "configure_search_vq.html": {
    "href": "configure_search_vq.html",
    "title": "Changing Visual Query conditions",
    "keywords": "Changing Visual Query conditions Modify the Visual Query conditions file in the i2 Analyze configuration, then redeploy the application server. Before you begin Ensure that you can connect to the deployment and submit data by using Analyst's Notebook Premium. To understand what Visual Query conditions are, and why you might need to modify the conditions, see Visual Query condition restrictions . Modifying the Visual Query conditions file The visual-query-configuration.xml file in the distributed deployment example is in the following directory: src/configuration/fragments/opal-services/WEB-INF/classes . In an XML editor, open the visual-query-configuration.xml file from the classes directory, and modify the Visual Query conditions. After you modify the file, save your changes. For more information about the Visual Query conditions file, see Visual Query condition restrictions . Updating the DiscoServerSettingsCommon.properties file The DiscoServerSettingsCommon.properties file in the distributed deployment example is in the following directory: src/configuration/fragments/opal-services/WEB-INF/classes . In a text editor, open the DiscoServerSettingsCommon.properties file and set the value of the VisualQueryConfigurationResource setting to visual-query-configuration.xml . For example: VisualQueryConfigurationResource=visual-query-configuration.xml . After you modify the file, save your changes. Copying the configuration Your Visual Query conditions file changes are required by Liberty. The modified configuration must be copied to the Liberty server. In a Docker environment, use the docker cp command to copy, and overwrite, your configuration to the running liberty container. In the src directory, run the following command: docker cp ./configuration/fragments/opal-services/WEB-INF/classes liberty:/opt/i2/i2analyze/toolkit/configuration/fragments/opal-services/WEB-INF/ The new configuration is now on the liberty container. When the directory is copied, the owner of the directory, and all files within it, is changed to root . The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following command: docker exec -u root liberty chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration In a non-Docker environment, copy the modified visual-query-configuration.xml file to the same location on the Liberty server and ensure that the correct permissions are set. Updating the application Run the following commands on the Liberty server to stop, deploy, and start Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t deployLiberty docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty Testing the deployment To test that the Visual Query conditions are updated successfully, connect to the Information Store in Analyst's Notebook Premium. After you connect and log in, complete a Visual Query search that demonstrates if your Visual Query condition changes are applied."
  },
  "configure_solr_add_node.html": {
    "href": "configure_solr_add_node.html",
    "title": "Adding another Solr server and Solr node",
    "keywords": "Adding another Solr server and Solr node You can add an extra Solr server to your distributed deployment example. This section describes the process of creating a new Docker container that represents the extra Solr server, and adding a Solr node on that server to the existing Solr collection. Before you begin Create a directory named solr3 in the src/images/common directory, and copy the contents of the src/images/common/solr directory into the new solr3 directory. Modifying the Dockerfile In the Docker environment, create another container to represent the extra server. The new container is a copy of the Solr container, which is modified to use a different host name and port. Modify the Dockerfile in the src/images/common/solr3 directory to specify the host name and port to expose: Set the ARG instruction to ARG hostname=solr3.eianet . This is the host name of the new container. Set the EXPOSE instruction to EXPOSE 8985 . This is the port that is exposed to enable a connection the Solr web UI. Adding the Solr node to the topology.xml Modify the topology.xml in the src/images/common/solr3/configuration/environment directory to include the new host name and ports that are specified in the Dockerfile . Add the following <solr-node> element as a child of the <solr-nodes> element: <solr-node memory=\"512m\" data-dir=\"/opt/i2/i2analyze/data/solr\" host-name=\"solr3.eianet\" id=\"node3\" port-number=\"8985\"/> Make the same modification to the topology.xml in the src/configuration/environment directory to ensure that the configuration is consistent. Build and create the Solr container Build the Solr image, run the following command from the src/images/common folder: docker build -t solr3_image solr3 The Solr image is created with the name solr3_image . Inspect the Dockerfile in the src/images/common/solr3 directory to see the commands that are required to configure the Solr server. Run the Solr3 container: docker run -d --name solr3 -p 8985:8985 --net eianet --memory=2g -u i2analyze solr3_image Check that the container started correctly by using the docker logs: docker logs -f solr3 The image includes the topology.xml file that you modified, and defines the values for the configuration of the new Solr node. When the server starts, the new Solr node is created on the new server. When you start the Solr container, the port that Solr runs on in the container is specified. The specified port is mapped on the host machine so that you can access Solr and the Solr Web UI from the host machine. After the Solr node is running, you can use the Solr Web UI to inspect the configuration. Connect to the Solr Web UI on the solr container. In a web browser, go to the following URL to connect to the Solr Web UI: http://localhost:8983/solr/# . The user name is solradmin and the password is the Solr password set in the credentials.properties file. What to do next After you create the Solr node, you can configure your Solr collection to use the new node by adding a shard or replica to the node. For more information, see Adding Solr shards to your deployment , and Adding Solr replicas to your deployment ."
  },
  "configure_solr_add_replica.html": {
    "href": "configure_solr_add_replica.html",
    "title": "Adding Solr replicas to your deployment",
    "keywords": "Adding Solr replicas to your deployment If a logical shard in the Solr collection becomes unusable, then the search index also becomes unusable. To increase the resilience of the search index, replicas of shards can be created that are distinct from their original shard. These replicas can be located on different physical servers. A replica acts as a physical copy of a shard in a SolrCloud Collection. After you change the Solr collection in a live deployment, replicate the changes in the definition of how the Solr collection is created. Adding replicas to the Solr collection definition After you identify how many replicas you require, add them to your Solr collection definition: If your Solr collection is configured by the createSolrCollection script, increase the replication factor for your collection in the CREATE call to the Solr Collections API. If your Solr collection is configured by the toolkit, increase the value of the num-replicas attribute for your collection in the topology.xml file. For example, if you want to have two replicas of each shard, increase the replicationFactor or num-replicas value to 2. The next time that the Solr collection is created by the toolkit, it is created with the replication factor that you specified in the topology.xml file or the createSolrCollection script. You only need to complete the following steps when you are adding replicas to a live system. If you do add replicas to the live system, ensure that you update the collection definition. Adding replicas to a live collection Add a replica of each shard in the Solr collection. To add a replica of each shard in the distributed deployment example, enter the following lines into the URL field in a web browser: http://localhost:8983/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=solr2.eianet:8984_solr http://localhost:8983/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard2&node=solr.eianet:8983_solr http://localhost:8983/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard3&node=solr2.eianet:8984_solr http://localhost:8983/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard4&node=solr.eianet:8983_solr These send a REST call to the Solr Collections API to add a replica of a specific shard on a specific node. Each of the shards now has an extra replica, on a different node to the original shard. For more information about the ADDREPLICA call, see ADDREPLICA: Add replicas . To prepare the REST calls, you must know the collection name, the shard to replicate, and the node to add the replica to. To identify the node name, go to the Tree View of SolrCloud in the Solr Web UI on any of your running Solr servers. For example: http://localhost:8983/solr/#/~cloud?view=tree Go to /live_nodes , and choose the name of the node that you want to move your replica to. For example: solr2.eianet:8984_solr Note: Only live Solr nodes are displayed. You can use this method to choose the number of replicas to create, and where they are located. To show the Graph View that shows replicas in the Solr Web UI, connect to the Solr Web UI by using the following URL: http://localhost:8983/solr/#/~cloud . The user name is solradmin and the password is the Solr password set in the credentials.properties file. Important: After you change the Solr collection in a live deployment, replicate the changes in the definition of how the Solr collection is created. Testing the replicas To test that the replicas are added successfully, and that your system has a level of redundancy, stop one of the Solr nodes. To stop the Solr node on the solr container, run the following command: docker exec -u i2analyze -t solr /opt/i2/i2analyze/toolkit/scripts/setup -t stopSolrNodes --hostname solr.eianet In the Solr Web UI Tree View on the solr2 container, all the shards are still up and active. The newly created replicas are now the leaders for the shards. If you do a Quick Search by using Analyst's Notebook Premium, any data that was in the Information Store is still available. You can start the solr container, and the Solr node starts again. To start the solr container, run the following command: docker exec -u i2analyze -t solr /opt/i2/i2analyze/toolkit/scripts/setup -t startSolrNodes --hostname solr.eianet In the Solr Web UI Tree View , you can see that the Solr node and shards are active."
  },
  "configure_solr_add_shard.html": {
    "href": "configure_solr_add_shard.html",
    "title": "Adding Solr shards to your deployment",
    "keywords": "Adding Solr shards to your deployment You can modify the Solr configuration by using the Solr Collections API . After you change the Solr collection in a live deployment, replicate the changes in the definition of how the Solr collection is created. Overview When you first deploy i2 Analyze, you should create a Solr collection that comprises enough shards to satisfy your future data requirements. If your data requirements change, you can add more shards to your deployment. Adding a shard to the Solr collection definition After you identify the number of shards that you require, add them to your Solr collection definition: If your Solr collection is configured by the createSolrCollection script, increase the number of shards for your collection in the CREATE call to the Solr Collections API. If your Solr collection is configured by the toolkit, increase the value of the num-shards attribute for your collection in the topology.xml file. The next time that the Solr collection is created by the toolkit, it is created with the number of shards that you specify. The following steps are only required when you want to add a shard to a live deployment. Adding a shard to a live collection In the Solr collections that are created by i2 Analyze, the compositeId router is used, which means that you cannot add shards explicitly. To add a shard to an existing collection, you must split an existing one. If you add a shard to a live collection, you must replicate the same changes to your Solr collection creation definition. Splitting a shard To split shard1 in the distributed deployment example, enter the following line into the URL field in a web browser: http://localhost:8983/solr/admin/collections?action=SPLITSHARD&collection=main_index&shard=shard1 This sends a REST call to the Solr Collections API to split shard1 . For more information about the SPLITSHARD call, see SPLITSHARD: Split a shard . The process of splitting a shard creates two new shards from the original shard. When a shard is split, the names of the two new shards that are created are the name of the original shard that is suffixed with _0 and _1 . The original shard becomes inactive as soon as the new shards and their replicas are up. The replication factor of the original shard is maintained. If your shard had two replicas, when it is split each new shard also has two replicas. Note: The new shards are always placed on the same node as the parent shard. Deleting the inactive shard You can remove the inactive shard1 from the collection. To delete the shard, enter the following line into the URL field in a web browser and press Enter : http://localhost:8983/solr/admin/collections?action=DELETESHARD&shard=shard1&collection=main_index This sends a REST call to the Solr Collections API to delete shard1 . For more information about the DELETESHARD call, see DELETESHARD: Delete a shard . You can now either test or the deployment, or move the new shard to another Solr node. Moving a shard to another node If you want to move one of the new split shards, you must create a replica of it on another node and then delete the original. To create the REST call to do this, you must know the collection name, the shard to replicate, and the node to add the replica to. To identify the node name, navigate to the Tree View of SolrCLoud in the Solr Web UI on any of your running Solr servers. For example: http://localhost:8983/solr/#/~cloud?view=tree Go to /live_nodes and choose the name of the node that you want to move your replica to. For example: solr3.eianet:8985_solr Note: Only live Solr nodes are displayed. To add a replica of the new shard1_1 shard to the new Solr node 3, paste the following line into your browser: http://localhost:8983/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1_1&node=solr3.eianet:8985_solr This sends a REST call to the Solr Collections API to create a replica of shard1_1 on solr3 that is on port 8985. For more information about the ADDREPLICA call, see ADDREPLICA: Add replicas . Deleting the original replica To delete the original replica that was created when the shard was split, you must have the name of the replica to remove. To get the replica name, go to the Tree View of SolrCLoud in the Solr Web UI. For example: http://localhost:8983/solr/#/~cloud?view=tree Go to /collections/main_index/state.json . This file describes the composition of your collection. Identify the replica to delete by looking at the shard that the replica replicates, and the node that the replica is located. In this example, the replica to remove is shards.shard1_1.replicas.core_node6 . To delete this replica, paste the following line into your browser: http://localhost:8983/solr/admin/collections?action=DELETEREPLICA&collection=main_index&shard=shard1_1&replica=core_node6 This sends a REST call to the Solr Collections API to delete the first replica of shard1_1 that was on node solr2 . For more information about the DELETEREPLICA call, see DELETEREPLICA: Delete a replica . Testing the deployment To test that the system continues to work, create and search for data by using Quick Search in Analyst's Notebook Premium."
  },
  "configure_solr_change_node_memory.html": {
    "href": "configure_solr_change_node_memory.html",
    "title": "Changing the memory allocation for a Solr node",
    "keywords": "Changing the memory allocation for a Solr node After you deploy your system, you might need to change the maximum Java virtual machine (JVM) memory allocation allowance for your Solr nodes. For more information about configuring the JVM memory allocation, see JVM Settings . Modifying the topology.xml Modify the topology.xml in the src/configuration/environment directory. For the Solr node that you want to change the memory allocation of, change the value of the memory attribute of the solr-node element. For example, the memory allocation for node1 , which runs on the solr container, is changed to 4g : <solr-nodes> <solr-node memory=\"4g\" data-dir=\"/opt/i2/i2analyze/data/solr\" host-name=\"solr.eianet\" id=\"node1\" port-number=\"8983\"/> <solr-node memory=\"512m\" data-dir=\"/opt/i2/i2analyze/data/solr\" host-name=\"solr2.eianet\" id=\"node2\" port-number=\"8984\"/> </solr-nodes> Copying the configuration The configuration was modified for the Solr node that runs on the solr server. You must copy the new configuration to the Solr server. In a Docker environment, use the Docker cp command to copy, and overwrite, your configuration to the running solr container. In the src directory, run the following command: docker cp ./configuration solr:/opt/i2/i2analyze/toolkit/ The new configuration is now on the solr container. When the directory is copied, the owner of the directory, and all files within it, is changed. The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following command: docker exec -u root solr chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration In a non-Docker environment, copy the modified topology.xml file to the same location in your i2 Analyze toolkit on the Solr server. Updating the Solr node Restart the Solr node with the new configuration. Restart the Solr node on the solr container by running the following command: docker exec -u i2analyze -t solr /opt/i2/i2analyze/toolkit/scripts/setup -t restartSolrNodes --hostname solr The Solr node is restarted with the new JVM maximum memory allocation. Testing the deployment You can use the Solr Web UI to inspect the Solr node that you modified to see the new memory allocation. To connect to the Solr Web UI of the Solr node on the solr container, go to: http://localhost:8983/solr/#/ . The user name is solradmin and the password is the Solr password set in the credentials.properties file. On the right side of the dashboard, see that the JVM-Memory has a value of 4GB ."
  },
  "configure_solr_collection.html": {
    "href": "configure_solr_collection.html",
    "title": "Creating a Solr collection using the createSolrCollection script file",
    "keywords": "Creating a Solr collection using the createSolrCollection script file Your createSolrCollection script in the toolkit\\configuration\\environment\\opal-server\\main_index directory must contain a CREATE call to the Solr Collections API. Before you begin You must specify that you are using the createSolrCollection script to configure Solr. For more information, see Choosing the method for configuring Solr . CREATE REST call To create a collection, use the CREATE REST call in the createSolrCollection file. For more information about the CREATE REST call, and extra parameters that you can use, see CREATE: Create a Collection . To create a collection, open the createSolrCollection script in a text editor. The existing script might contain a CREATE call that is similar to the following example: curl ?CERTIFICATE_PLACEHOLDER? ?CREDENTIALS_PLACEHOLDER? \"?SCHEME_PLACEHOLDER?://solr.eianet:8983/solr/admin/collections?action=CREATE&name=main_index&collection.configName=main_index&numShards=4&maxShardsPerNode=4&replicationFactor=1\" The ?CERTIFICATE_PLACEHOLDER?, ?CREDENTIALS_PLACEHOLDER?, ?SCHEME_PLACEHOLDER? variables are resolved by the toolkit when the script is run. This example of a CREATE call creates a collection that has the following attributes: Named main_index A configuration named main_index in ZooKeeper Four shards A maximum of four shards per node One replica of each shard Recreating the collection After you change the createSolrCollection script, you can recreate the Solr collection by completing the following steps. To stop Liberty, run the following command: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty The output from the stopLiberty task is output directly to the console. To remove the current Solr collection, run the following command: docker exec -it -u i2analyze admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t deleteSolrCollections --all --hostname admin_client.eianet To create the Solr collection with the modified configuration, run the following command: docker exec -t -u i2analyze admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createSolrCollections --all --hostname admin_client.eianet To clear the search index, run the following command: docker exec -u i2analyze -it admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t clearSearchIndex --all --hostname admin_client.eianet To restart Liberty, run the following command: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. Liberty is now started and you can connect to i2 Analyze. Modifying a collection If you change the configuration of your Solr collection on a live deployment, you must replicate the changes in your createSolrCollection script. This ensures that if your Solr collection is recreated, it aligns with your configuration changes. For example, if you add two shards to your live collection, you must increase the number of shards in the collection definition by 2. For information about changing the Solr collection configuration on a live deployment, see Adding another Solr server and Solr node , Adding Solr shards to your deployment , and Adding Solr replicas to your deployment . Note: If you add Solr nodes to your deployment after the Solr Collection is created, your CREATE call might distribute your original shards and replicas on different nodes. You must inspect your createSolrCollection script and ensure that it works if your Solr collection is created with the new nodes present from the start."
  },
  "configure_solr_method.html": {
    "href": "configure_solr_method.html",
    "title": "Choosing the method for configuring Solr",
    "keywords": "Choosing the method for configuring Solr A deployment of i2 Analyze uses Solr to manage the search index. i2 Analyze requires at least one Solr collection that consists of nodes, shards, and replicas. The Solr configuration can be controlled through the topology.xml file, or through the createSolrCollection script. Regardless of how Solr is configured, the collection is created by i2 Analyze. If the collection is configured through the topology.xml file, the modifications that you can complete are limited to those that are supported in the topology.xml file. If the collection is managed through the createSolrCollection script, you can complete all the modifications that are supported by the Solr Collections API. The createSolrCollection script file contains REST calls to the Solr Collections API to configure the Solr collection. The script uses curl commands to make the REST calls. Note: You must install curl on your path to use the createSolrCollection script as it is generated. Configure the Solr collection in the topology.xml file By default, or when toolkit-configured=\"true\" in the solr-collection element of the topology.xml file, the Solr collection is created by using the definitions in the topology.xml file. For example, the <solr-collection> element is defined as follows: <solr-collection toolkit-configured=\"true\" num-replicas=\"1\" id=\"main_index\" num-shards=\"4\" lucene-match-version=\"\"/> For more information about configuring the collection and the values that you can set in the topology file, see ZooKeeper and Solr . The Solr collection is created when the createSolrCollections toolkit task is run the first time. After, you can use the clearSearchIndex toolkit task. This task clears the search index, and then recreates the collection. When Liberty starts, the search index is recreated. Configure the Solr collection in the createSolrCollection script file If you deployed the distributed deployment example, the createSolrCollection script is in the toolkit\\configuration\\environment\\opal-server\\main_index directory on the server that the createSolrCollection script was run. You can then change the topology file to toolkit-configured=\"false\" . Note: If the script doesn't exist, run setup -t createSolrCollections toolkit task with toolkit-configured=\"true\" in the topology.xml file. If toolkit-configured=\"false\" in the topology.xml file, the createSolrCollection script is used to configure the Solr collection. For example, the <solr-collection> element is defined as follows: <solr-collection toolkit-configured=\"false\" num-replicas=\"1\" lucene-match-version=\"\" id=\"main_index\" num-shards=\"4\"/> The script file contains the Solr collection configuration from the topology.xml file. You can then modify the Solr Collections API CREATE call in the script file. When i2 Analyze recreates the Solr collection, it uses the call in this file. For more information about configuring the collection by using the script, see Creating a Solr collection using the createSolrCollection script . Managing changes to the createSolrCollection script file The script file contains the definition of your Solr collection. You must keep a copy of the createSolrCollection in a source control system to ensure that the REST call required to make your Solr collection is maintained. You might need to use previous versions of this file when you move from a test to a production system, or when you make a configuration change that you later decide is incorrect."
  },
  "configure_system_clear_data.html": {
    "href": "configure_system_clear_data.html",
    "title": "Clearing data from the system",
    "keywords": "Clearing data from the system During development, it is common to need to clear data from the i2 data stores. Before you begin This process can permanently remove data from your system, ensure that no data is in the system that you want to keep. For information about making a backup of your deployment, see Backing up a deployment . Stopping Liberty Run the following command on the Liberty server to stop Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty The console output from the stopLiberty task is output directly to the console. Clear data To remove data from both the search index and the database, you can use the clearData toolkit task. For more information, see Clearing data from the system . To clear the data, you can run the command from the Admin client or Liberty server. In the distributed deployment example, the admin_client container is used to run the command. The admin_client container includes the i2 Analyze toolkit and a database management client installation. You must run commands that modify the database and Solr and ZooKeeper from this container. To clear the data, run the following command: docker exec -u i2analyze -it admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t clearData --hostname admin_client.eianet The database is now clear of data and the search index is now empty. The database still exists with the structure from the schema. If you want to make destructive schema changes, follow the instructions in Resetting the system . Restarting your system To restart Liberty, run the following command: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. Liberty is now started and you can connect to i2 Analyze. Testing the deployment Connect to i2 Analyze and search for data. If you perform a wildcard search for * , all of the data in your system is returned. After you run clearData , this search returns no results."
  },
  "configure_system_reset.html": {
    "href": "configure_system_reset.html",
    "title": "Resetting the system",
    "keywords": "Resetting the system During development, to make destructive schema changes or to completely change your schema, you must recreate the Information Store database. Before you begin This process can permanently remove data and the database from your system, ensure that no data is in the system that you want to keep. For more information about making a backup of your deployment, see Backing up a deployment . Stopping Liberty Run the following command on the Liberty server to stop Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty The console output from the stopLiberty task is output directly to the console. Clearing the search index To clear the search index, you can run the commands from the Admin client or Liberty server. In the distributed deployment example, the admin_client container is used to run the command. The admin_client container includes the i2 Analyze toolkit and a database management system client installation. You can run commands that modify the database and Solr from this container. To clear the search index, run the following command: docker exec -u i2analyze -it admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t clearSearchIndex --all --hostname admin_client.eianet The search index is cleared, however any data that was in the database remains in the database. Removing databases from the system To remove the database from your system, you can use the dropDatabases toolkit task. For more information, see Removing databases from the system . To drop the database, you must ensure that there are no active connections to it. Stopping active connections to your SQL Server database You can identify the active connections to the database by running the following command: docker exec sqlserver /opt/mssql-tools/bin/sqlcmd -S localhost -U i2analyze -P Passw0rd -Q \"EXEC SP_WHO2\" If the command returns no results, or there are no rows in the list with a DBName of ISTORE , there are no active connections to the Information Store. Otherwise, make a note of all the values in the SPID column with a DBName of ISTORE . The following is an example of how to stop an active connection with the SPID of 54 : docker exec sqlserver /opt/mssql-tools/bin/sqlcmd -S localhost -U i2analyze -P Passw0rd -Q \"KILL 54\" Any connections that you stop in this way are not in the list when you run the command to identify the active connections. Stopping active connections to your Db2 database You can identify the active connections to the database by running the following command: docker exec -u i2analyze -t db2 bash -c \". /home/db2inst1/sqllib/db2profile && db2 list applications\" If the command returns the No data was returned by Database System Monitor warning, or there are no rows in the list with a DB Name of ISTORE , there are no active connections to the Information Store. Otherwise, make a note of all of the values in the Appli. Handle column with a DB Name of ISTORE . The following is an example of how to stop two active connections with the application handles of 1414 and 1415 : docker exec -u i2analyze -t db2 bash -c \". /home/db2inst1/sqllib/db2profile && db2 'force application (1414, 1415)'\" Any connections that you stop in this way are not in the list when you run the command to identify the active connections. Stopping active connections to your Postgres database You can identify the active connections to the database by running the following command: docker exec -e PGPASSWORD=Passw0rd postgres bash -c \"psql -w -X -q --set=client_min_messages=warning -h postgres -p 5432 -d ISTORE -U i2analyze -c 'select datname, pid, usename from pg_stat_activity;'\" If the command returns no results, or there are no rows in the list with a datname of ISTORE , there are no active connections to the Information Store. Otherwise, make a note of all the values in the pid column with a datname of ISTORE . The following is an example of how to stop an active connection with the pid of 185 : docker exec -e PGPASSWORD=Passw0rd postgres bash -c \"psql -w -X -q --set=client_min_messages=warning -h postgres -p 5432 -d ISTORE -U i2analyze -c 'select pg_cancel_backend(185);'\" docker exec -e PGPASSWORD=Passw0rd postgres bash -c \"psql -w -X -q --set=client_min_messages=warning -h postgres -p 5432 -d ISTORE -U i2analyze -c 'select pg_terminate_backend(185);'\" Any connections that you stop in this way are not in the list when you run the command to identify the active connections. Drop the database After you ensure that there are no active connections to your database, you can drop the database by running the following command: In SQL Server: docker exec -u i2analyze -it admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t dropDatabases --hostname admin_client.eianet In Db2: docker exec -u i2analyze -it admin_client bash -c \". /home/db2inst1/sqllib/db2profile && /opt/i2/i2analyze/toolkit/scripts/setup -t dropDatabases --hostname admin_client.eianet\" In Postgres: docker exec -u i2analyze -it admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t dropDatabases --hostname admin_client.eianet The ISTORE database is removed. Recreating the database Before you can restart i2 Analyze, you must recreate the database. To create the database, run the following command: In SQL Server: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createDatabases In Db2: docker exec -u i2analyze -t admin_client bash -c \". /home/db2inst1/sqllib/db2profile && /opt/i2/i2analyze/toolkit/scripts/setup -t createDatabases\" In Postgres: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createDatabases You must start Liberty: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. Liberty is now started and you can connect to i2 Analyze. Testing the deployment Connect to i2 Analyze and search for data. If you perform a wildcard search for * , all of the data in your system is returned. After you remove the database from the system, this search returns no results."
  },
  "deploy_change_database_management_system.html": {
    "href": "deploy_change_database_management_system.html",
    "title": "Change the database management system of your example",
    "keywords": "Change the database management system of your example You can deploy the distributed example with the Information Store on IBM Db2, Microsoft SQL Server or Postgres. After you deploy your example, you can change the configuration so that the database is on the other management system. When you run the script that changes the database management system of the deployment, the following tasks are completed: The required Docker images are built The required Docker containers are run The i2 Analyze configuration is updated The Information Store database is created i2 Analyze is deployed and started This is process is not representative of an activity that you should complete in a non-Docker environment. In a non-Docker environment you must remove the initial deployment, and redeploy with your chosen database management system. The following process must only be used in the example Docker environment. Note: If you deployed the example with one of the configurations in the configuration_mods directory or changed the topology.xml file. Reset your example deployment to the base configuration before you deploy with Db2. To reset your environment, run the following command from the src/scripts directory: ./resetEnvironment To change the database management system to use IBM Db2, see Change the deployment to use IBM Db2 . To change the database management system to use Microsoft SQL Server, see Change the deployment to use Microsoft SQL Server . To change the database management system to use Postgres, see Change the deployment to use Postgres . Change the deployment to use IBM Db2 Prerequisites for the distributed deployment example with IBM Db2 Download IBM Db2 Advanced Edition for Linux by using the usual mechanism. Unzip the DB2_DAE_Activation_11.5.zip file, then copy the adv_vpc directory into the src/images/db2/db2_installer directory. Rename the DB2_Svr_11.5_Linux_x86-64.tar.gz file to DB2_AWSE_REST_Svr.tar.gz , then copy it to the src/images/db2/db2_installer/installation_media directory. Do not decompress the file. Download the SSL support file for Db2 by using the usual mechanism. Rename the DB2_SF_SSL_11.5_Linux_x86-64.tar.gz file to DB2_SF_SSLF.tar.gz , then copy it to the src/images/db2/base_client/installation_media directory. Do not decompress the file. Add the db2jcc4.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: In an installation of Db2, the db2jcc4.jar file is in the IBM/SQLLIB/java directory. If you do not have a Db2 installation, you can download the file. Download the file for v11.5 FP0 (GA) . For more information about downloading the db2jcc4.jar file, see: http://www-01.ibm.com/support/docview.wss?uid=swg21363866 . Run the script to deploy with Db2 Navigate to the src/scripts directory, and run the following command: ./deployDb2 Change the deployment to use Microsoft SQL Server When you complete the quick deployment of the distributed example, the prerequisites for SQL Server are configured. To change the deployment to use SQL Server, navigate to the src/scripts directory and run the following command: ./deploySqlServer Change the deployment to use Postgres Prerequisites for the distributed deployment example with Postgres Add the postgresql-xx.x.x.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: Download the latest (Java 8) PostgreSQL JDBC Driver for PostgreSQL from https://jdbc.postgresql.org/download/ . Run the script to deploy with Postgres Navigate to the src/scripts directory, and run the following command: ./deployPostgres Test the deployment To test the deployment, connect to i2 Analyze from Analyst's Notebook Premium. The URL that you use to connect is: http://i2demo:9082/opal . Log in using the user name Jenny with the password Jenny . Use the Upload records functionality to add data to the Information Store, and then search for that data."
  },
  "deploy_clean_environment.html": {
    "href": "deploy_clean_environment.html",
    "title": "Cleaning your environment",
    "keywords": "Cleaning your environment Docker containers, images, and networks must have unique names. To recreate the distributed deployment example, or modify images and containers, you must remove the existing ones before you can build and run new ones. clean script You can run the src/scripts/clean file to clean your environment. The clean file removes all of the example containers, images, and the network from your environment. The following sections describe in detail how to clean your environment manually. Removing containers To remove containers, run the Docker rm command and specify the name of the containers to remove. For example, run the following command: docker rm -f container_name To remove all of the containers that are used in the distributed deployment example, run the following command: docker rm -f zookeeper zookeeper2 zookeeper3 solr solr2 solr3 db2 sqlserver postgres liberty admin_client etl_client ca connector Attempting to remove containers that do not exist errors but does not stop the rest of the existing containers from being removed. You can now restart any of the containers that were removed. Removing images After all of the containers that are running the images are removed, you can remove the images. To remove images, run the Docker rmi command and specify the names of the images to remove. For example: docker rmi -f image_name To remove the images that are used in the distributed deployment example, run the following commands: docker rmi -f zookeeper_image zookeeper2_image zookeeper3_image solr_image solr2_image solr3_image ca_image connector_image docker rmi -f liberty_sqlserver_image liberty_db2_image liberty_postgres_image etl_client_db2_image etl_client_sqlserver_image etl_client_postgres_image docker rmi -f sqlserver_image db2_installer_image db2_image postgres_image admin_client_sqlserver_image admin_client_db2_image admin_client_postgres_image base_client_image docker rmi -f ubuntu_toolkit_image Note: If the image that you want to remove is used to derive other images, you must remove all of the derived images before you can remove the image. If you try to remove an image that does not exist, the command results in an error but other images are still removed. After you remove an image, you can rebuild it. If you want to also remove the image dependencies, you can run: docker rmi -f ubuntu Removing the network After all of the containers are removed, you can remove the network. To remove the network, run the docker network command with the rm flag and specify the network name to remove. For example, run the following command: docker network rm network_name To remote the network that is used in the distributed deployment example, run the following command: docker network rm eianet The eianet network is removed."
  },
  "deploy_etl_client.html": {
    "href": "deploy_etl_client.html",
    "title": "Ingesting data remotely",
    "keywords": "Ingesting data remotely i2 Analyze supports physical architectures in which the database is hosted on the same server as the application, or on a different one. You can also choose to locate your Extract Transform Load (ETL) logic on the same server as the i2 Analyze application, or on the same server as the database, or on an entirely separate server. The distributed deployment example shows you how to set up the ETL logic on a separate server. For more information about the ingestion architecture and the ETL toolkit, see Understanding the architecture . Before you begin Ensure that you can connect to the deployment and submit data by using Analyst's Notebook Premium. Deploying the ETL toolkit In the distributed deployment example, the ETL toolkit is deployed in its own server. Your configured i2 Analyze toolkit and database client must be installed on the ETL toolkit server. The createEtlToolkit toolkit task creates the ETL toolkit. In the Docker environment, the ETL toolkit is created inside the admin_client container and then copied into the ETL client container. In the Docker environment, the etl_client_sqlserver_image is created when you run the buildImages script. The ETL client container is started when you run the runContainers script. If you cleaned your environment, rebuild and rerun the container. To build the ETL client image, run the following command from the src/images folder: For SQL Server: docker build -t etl_client_sqlserver_image sqlserver/etl_client For Db2: docker build -t etl_client_db2_image db2/etl_client For Postgres: docker build -t etl_client_postgres_image postgres/etl_client To run the ETL client container, run the following command: For SQL Server: docker run -d --name etl_client --net eianet -u i2analyze etl_client_sqlserver_image For Db2: docker run -d --name etl_client --net eianet -u i2analyze etl_client_db2_image For Postgres: docker run -d --name etl_client --net eianet -u i2analyze etl_client_postgres_image Run the deployEtlClient script from the src/scripts directory to create a new ETL toolkit and copy it to the etl_client container: ./deployEtlClient For more information about deploying the ETL toolkit in a non-Docker environment, see Deploying the ETL toolkit . Configuring the ETL toolkit for Db2 The etl_client container includes the initializeEtlClient script that you must run in the Docker environment when you are using Db2. The initializeEtlClient script catalogs the remote Db2 node and database with the Db2 Client that is installed on the ETL client. To complete this process in a non-Docker environment, see Deploying the ETL toolkit . Run the initializeEtlClient script on the etl_client container: docker exec -u i2analyze -t etl_client /opt/i2/i2analyze/initializeEtlClient You can find the initializeEtlClient script in the src/images/db2/etl_client directory. If the connection to Db2 uses SSL, run the following command: docker exec -u i2analyze -t etl_client /opt/i2/i2analyze/initializeEtlClient enable . Ingesting the example data The ETL client container includes the ingestExampleData script that you can run in the Docker environment to ingest example data into the Information Store. You can now use the ETL toolkit on the ETL client server to ingest data into the Information Store. The ingestExampleData script in the Docker container ingests example data into the Information Store. To complete this process in a non-Docker environment, see Running ingestion commands . Run the ingestExampleData script on the etl_client . Type the password that you set in the credentials.properties file for Information Store database: docker exec -u i2analyze -it etl_client /opt/i2/i2analyze/ingestExampleData An E_Person entity staging table is created and populated, and the data that it contains is ingested. An L_Associate link staging table is created and populated, and the data that it contains is ingested. You can find the ingestExampleData script in the etl_client directory. Testing the deployment To test that the data ingested successfully, connect to the Information Store in Analyst's Notebook Premium and search for Jo Black and complete an expand operation. You should see a link to another entity named John Smith ."
  },
  "deploy_i2_connect.html": {
    "href": "deploy_i2_connect.html",
    "title": "Deploying i2 Analyze with i2 Connect",
    "keywords": "Deploying i2 Analyze with i2 Connect i2 Connect enables analysts to search for and retrieve data from external data sources by using the Opal quick search functionality, and then analyze the results on a chart in Analyst's Notebook Premium. You can deploy i2 Analyze with support for i2 Connect only, or with the Information Store and i2 Connect. For more information about deploying i2 Analyze with i2 Connect, see i2 Analyze and i2 Connect . Before you begin You must complete the Quick deploy or deploy the example manually, and your deployment must be running. If you deployed the example with one of the configurations in the configuration_mods directory or changed the topology.xml file. Reset your example deployment to the base configuration before you deploy i2 Analyze with i2 Connect. To reset your environment, run the following command from the src/scripts directory: ./resetEnvironment Creating the keystores and certificates Run the createKeysAndStores script to create the stores, certificates, and certificate authority. For example, run the following command from the src/scripts directory: ./createKeysAndStores You are prompted to enter a password that is used for each of the keystores and truststores that are created. The password that you specify here is used later. For more information about the stores and certificates that are created, see Keystores and certificates for components of i2 Analyze . Deploying the example connector In the distributed deployment example, the example connector is deployed in its own server. In the Docker environment, the connector_image is created when you run the buildImages script. If you ran the clean script, rebuild and rerun the container. Otherwise, skip to the next step to run the connector container. To build the connector image, run the following command from the src/images folder: docker build -t connector_image common/connector The connector image is created with the name connector_image . To run the connector container, run the following command: docker run -d --name connector --net eianet --memory=512m \\ --net-alias \"connector.eianet\" \\ -e \"CONNECTOR_ID=example-connector\" \\ -e \"SSL_ENABLED=true\" \\ -e \"SSL_PRIVATE_KEY=<ssl_private_key>\" \\ -e \"SSL_CERTIFICATE=<ssl_certificate>\" \\ -e \"SSL_CA_CERTIFICATE=<ssl_ca_certificate>\" \\ -e \"SSL_PASSPHRASE=<ssl_passphrase>\" \\ -e \"SSL_GATEWAY_CN=liberty.eianet\" \\ -e \"SSL_SERVER_PORT=3443\" \\ \"connector_image\" Where the values are: ssl_passphrase should match the value that you entered for i2 Connect connector key store and certificate signing request when you ran the createKeysAndStores script. ssl_private_key is the content of the ./keys/stores/i2-connector-key.key file. ssl_certificate is the content of the ./keys/responses/connector-key.cer file. ssl_ca_certificate is the content of the ./images/common/ca/ca/CA.cer file. Specifying the credentials You must specify the credentials for your deployment in the src/configuration/environment/credentials.properties . Set the passwords to use for each of the keystore and truststore credentials. The passwords must match the values that you entered for the corresponding keystores and truststores when you ran the createKeysAndStores script. For more information about the credentials file, see Modifying the credentials . You can complete a quick i2 Connect deployment or you can complete the i2 Connect setup manually. Completing the i2 Connect deployment manually requires you to run each of the steps that were completed automatically in the quick i2 Connect deployment so that you can replicate the steps in a non-Docker environment. Quick deploy To set up the example connector in the distributed deployment example, a script is provided that deploys i2 Analyze with i2 Connect and the example connector. You can decide whether to deploy i2 Analyze with i2 Connect only, or with the Information Store and i2 Connect. To deploy with i2 Connect only, you must specify the i2connect configuration modification when you run the deployDaod script. To deploy with the Information Store and i2 Connect, you must specify the i2connect_istore configuration modification when you run the deployDaod script. Run the deployDaod script from the src/scripts directory to set up the i2 Analyze deployment with i2 Connect only and start the connector with the correct configuration. ./deployDaod i2connect For more information about deploying a connector for i2 Connect in a non-Docker environment, see Creating a connector for i2 Connect . Deploy example connector manually Stopping Liberty Run the stopLiberty task on the liberty container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty Configuring i2 Analyze Configure i2 Analyze to use i2 Connect. In the distributed deployment example, you can see the configuration modifications in the src/configuration_mods/<database management system>/i2connect directory. The updateServerConfigurations script copies a configuration to each of the example containers. To copy the i2 Connect only configuration, run the following command: ./updateServerConfigurations i2connect If you want to deploy i2 Analyze with the Information Store and i2 Connect, run updateServerConfigurations i2connect_istore . Deploying i2 Analyze Run the deployLiberty task on the liberty container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t deployLiberty Creating the Solr configuration on ZooKeeper You can create and upload the Solr configuration from the Admin client, or you can run the command from one of the ZooKeeper servers. To create the Solr configuration by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createAndUploadSolrConfig --hostname admin_client.eianet The Solr configuration is created and uploaded. Creating the Solr Collection Create the Solr Collection that is used by a deployment with i2 Connect, run the createSolrCollections task on the admin_client container. docker exec -t -u i2analyze admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createSolrCollections --hostname admin_client.eianet Starting Liberty To start Liberty, run the following command on the liberty container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. Results All the setup is completed and the application is started. Testing the deployment To test the deployment was configured successfully, connect to i2 Analyze from Analyst's Notebook Premium. If you deployed i2 Analyze with i2 Connect only, the URL that you use to connect is: http://i2demo:9082/opaldaod . If you deployed i2 Analyze with the Information Store and i2 Connect, the URL that you use to connect is: http://i2demo:9082/opal . Log in using the user name Jenny with the password Jenny . You can use the Search Stores functionality to search for data and query the external source."
  },
  "deploy_quick_start.html": {
    "href": "deploy_quick_start.html",
    "title": "Quick deploy",
    "keywords": "Quick deploy To deploy the distributed deployment example, scripts are provided that copy any prerequisites and build the docker images, then run the docker containers automatically. Important : The distributed example uses a Docker environment to demonstrate a distributed deployment of i2 Analyze. The Docker environment is not designed to be used on customer sites for test or production systems. After you understand how the distributed deployment is deployed, replicate the deployment on physical servers. Note: To deploy the distributed deployment example manually with detailed explanation about all of the steps, see Deploying the example manually on SQL Server , Deploying the example manually on Db2 and Deploying the example manually on Postgres . Note: If you deployed the distributed deployment example previously, ensure that you clean your environment before you start the manual deployment. For more information about how to clean your system, see Cleaning your environment . Prerequisites for the distributed deployment example Download, install, and configure the prerequisites for the distributed deployment example. Bash shell The scripts that are used in the distributed deployment example are Shell scripts. If you are on Windows , you can use WSL 2 as the backend for Docker and to run the shell scripts in this repository. To install WSL, in an administrator Command Prompt, run: wsl --install Then, restart your machine. For information about installing WSL, see Install Linux on Windows with WSL . Configure your WSL user. For more information, see Set up your Linux username and password . Npm The scripts that are used in the distributed deployment example requires npm to be installed in the bash shell. For more information, see Downloading and installing Node.js and npm . Docker You must install Docker CE for your operating system. For more information about installing Docker CE, see https://docs.docker.com/engine/install/ . Install Docker CE for your operating system. For more information about installing Docker CE, see https://docs.docker.com/engine/installation/ . Mac OS : Install Docker CE Windows : Install Docker CE Set up Docker on WSL 2 After you install Docker, allocate at least 5GB of memory to Docker to run the containers in the example deployment. On Windows, Docker is automatically allocated 8GB or 50% of available memory whichever is less. For more information about modifying the resources allocated to Docker, see: Docker Desktop for Windows Docker Desktop for Mac To test that your Docker command line environment is initialized correctly, run the following command: docker run hello-world Example code Clone or download the distributed deployment example from https://github.com/i2group/analyze-deployment/releases . Analyst's Notebook Download i2 Analyst's Notebook version 10 by using the usual mechanism. Install Analyst's Notebook with the Opal connector on a Windows machine that can access the machine where Docker is running. Note: If you are running Docker on Mac OS, you can install Analyst's Notebook Premium on a Windows virtual machine. For more information, see Installing i2 Analyst's Notebook . i2 Analyze Download i2 Analyze for Linux. You download the I2A_V4.4.3_LINUX.tar.gz version 4.4.3 by using the usual mechanism. Rename the .tar.gz file to i2analyze.tar.gz , then copy it to the src/images/common/ubuntu_toolkit/i2analyze directory. Accept the i2 Analyze license by opening license_acknowledgment.txt in the src/images/common/ubuntu_toolkit/i2analyze directory and changing the value of LIC_AGREEMENT to ACCEPT . Add the mssql-jdbc-7.4.1.jre11.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: Download the Microsoft JDBC Driver 7.4 for SQL Server from Microsoft JDBC Driver 7.4 for SQL Server . Extract the contents of the tar.gz , and locate the sqljdbc_7.4\\enu\\mssql-jdbc-7.4.1.jre11.jar file. Add the postgresql-xx.x.x.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: Download the latest (Java 8) PostgreSQL JDBC Driver for PostgreSQL from https://jdbc.postgresql.org/download/ Specifying the credentials You must specify the credentials for a deployment in the src/configuration/environment/credentials.properties . Set the passwords to be used for the Information Store, Solr, and the LTPA keys. The password that you specify for the Information Store must be at least 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols. For more information about the credentials file, see Modifying the credentials . To complete the quick deployment, enter passwords for the db.infostore.password , solr.password , and ltpakeys.password credentials. Build the Docker images In the src/scripts directory, run the buildImages file: ./buildImages The first time that you run this script, it might take about 15 minutes to complete. After, a cache is used and this process is quicker. Check that the images built correctly by using the Docker images command: docker images The following images must be listed: sqlserver_image , zookeeper_image , zookeeper2_image , zookeeper3_image , solr_image , solr2_image , admin_client_sqlserver_image , liberty_sqlserver_image , and ubuntu_toolkit_image . Run the Docker containers In the src/scripts directory, run the runContainers file: ./runContainers You are prompted to enter a password for the SA user, this is the system administrator user for the SQL Server container. The password that you specify must be at least 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols. You are prompted to enter a password for the i2analyze user. Enter the password that you specified in the credentials.properties file for the db.infostore.password credential. After the script in the runContainers file completes, all the containers are run and i2 Analyze is deployed. Test the deployment To configure Windows to connect to the Docker container by using the specified URL, you must change the hosts file. Open a command prompt as ADMINISTRATOR, and navigate to the following location: C:\\Windows\\System32\\drivers\\etc . Open the hosts file in a text editor. At the end of the file, add your IP Address followed by i2demo . For example: 127.0.0.1 i2demo NOTE : You must run Analyst's Notebook Premium on Windows. If you deploy the distributed deployment example on MAC, you can use a Windows virtual machine to host Analyst's Notebook Premium. For your virtual machine to connect to i2 Analyze, complete the following: On your MAC terminal, run ifconfig and identify the IP address for your virtual machine in a section such as vmnet1 . For example, 172.16.100.1 . Then, on your Windows virtual machine add the following line to the C:\\Windows\\System32\\drivers\\etc\\hosts file: 172.16.100.1 i2demo To test the deployment, connect to i2 Analyze from Analyst's Notebook Premium. The URL that you use to connect is: http://i2demo:9082/opal . Log in using the user name Jenny with the password Jenny . Use the Upload records functionality to add data to the Information Store, and then search for that data. What to do next To understand the distributed deployment example, what is deployed, and how it is deployed, complete the detailed instructions in deploying the example manually ."
  },
  "deploy_walk_through_db2.html": {
    "href": "deploy_walk_through_db2.html",
    "title": "Deploying the distributed deployment example manually for Db2",
    "keywords": "Deploying the distributed deployment example manually for Db2 The manual deployment of the distributed deployment example requires you to run each of the steps that were completed automatically in the Quick deploy, but for IBM Db2. The following instructions detail what steps are required on each container, and how to use these steps to deploy i2 Analyze on physical servers. Important : The distributed example uses a Docker environment to demonstrate a distributed deployment of i2 Analyze. The Docker environment is not designed to be used on customer sites for test or production systems. After you understand how the distributed deployment is deployed, replicate the deployment on physical servers. Before you begin Complete the Quick deploy section at least once, before you start the manual deployment. The software prerequisites that you download for the Quick deploy are also used here. Complete the Clean your environment section to ensure that none of the images, containers, or network exist. You must run all Docker commands from a command line where Docker is initialized. Db2 Download IBM Db2 Advanced Edition for Linux by using the usual mechanism. Unzip the DB2_DAE_Activation_11.5.zip file, then copy the adv_vpc directory into the src/images/db2/db2_installer directory. Rename the DB2_Svr_11.5_Linux_x86-64.tar.gz file to DB2_AWSE_REST_Svr.tar.gz , then copy it to the src/images/db2/db2_installer/installation_media directory. Do not decompress the file. Download the SSL support file for Db2 by using the usual mechanism. Rename the DB2_SF_SSL_11.5_Linux_x86-64.tar.gz file to DB2_SF_SSLF.tar.gz , then copy it to the src/images/db2/base_client/installation_media directory. Do not decompress the file. Add the db2jcc4.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: In an installation of Db2, the db2jcc4.jar file is in the IBM/SQLLIB/java directory. If you do not have a Db2 installation, you can download the file. Download the file for v11.5 FP0 (GA) . For more information about downloading the db2jcc4.jar file, see: http://www-01.ibm.com/support/docview.wss?uid=swg21363866 . Create the network To enable communication between the docker containers, all the containers must be connected to a single network. You can create a bridge network that only the docker containers can access. When each container is run, the --net flag is used to specify that the container connects to a specific network. Create a bridge network called eianet for the distributed deployment example: docker network create eianet Test that the network is created: docker network ls The eianet network is displayed in the list. In a non-Docker environment, ensure that each server that you are using to deploy i2 Analyze can connect to each other. Copy the i2 Analyze configuration Update the distributed deployment example configuration for Db2. Copy the src/configuration_mods/db2/base/environment directory to the src/configuration directory. Accept any file overwrites. The i2 Analyze configuration is required by all servers that host components of i2 Analyze, except for the database server. In the src/scripts directory of the distributed deployment example, the copyConfiguration script copies the i2 Analyze configuration from the src/configuration directory to the src/images/<container_name>/configuration directory for each container. From the src/scripts directory, run the copyConfiguration script file. ./copyConfiguration db2 Results In a non-Docker environment, copying the configuration is equivalent to the following steps: Downloading and extracting the i2 Analyze toolkit onto a server. Copying the examples/configurations/information-store-opal/configuration directory into the toolkit directory of your extracted i2 Analyze toolkit. You must populate the configuration for your environment, use the src/configuration directory as a reference. This is the centralized i2 Analyze toolkit and configuration that you copy to your servers. Build the prerequisite images In the distributed deployment example, some of the images are built on top of other images. The prerequisite images must be built first, as described in the following section. Ubuntu toolkit image The ubuntu_toolkit_image is an image that contains the Ubuntu operating system with i2 Analyze installed and contains the i2 Analyze configuration. When you build the ubuntu_toolkit_image , the i2analyze.tar.gz that you copied into the src/images/common/ubuntu_toolkit/i2analyze directory as part of the Quick deploy is copied and extracted into the image. To build the Ubuntu toolkit image, run the following from the src/images/common folder: docker build -t ubuntu_toolkit_image ubuntu_toolkit The Ubuntu toolkit image is created with the name ubuntu_toolkit_image . Db2 installer image The Db2 installation files must be added to the db2_installer_image . To build the db2_installer image, run the following from the src/images/db2 folder: docker build -t db2_installer_image db2_installer The db2_installer image is created with the name db2_installer_image . The prerequisite packages and Db2 installer files are installed. Base client image The base_client represents a system that has the configured i2 Analyze toolkit and Db2 Client. This is used by any container that must communicate with Db2. To build the base_client_image image, run the following from the src/images/db2 folder: docker build -t base_client_image base_client Configuring and running the containers Each Docker container requires a Docker image. In a non-Docker environment, this is equivalent to configuring and starting a physical server that is used to host a component of i2 Analyze. Db2 container In i2 Analyze, the Information Store is a Db2 database. To make it easier to demonstrate, the installation of Db2 is automated. In this deployment, Db2 is installed on its own server. To build the db2 image, run the following from the src/images/db2 folder: docker build -t db2_image db2 The DB2 image is created with the name db2_image . Run the db2 container: docker run -d --privileged --name db2 -p 50000:50000 -p 50001:50001 --net eianet -u db2inst1 db2_image Note: Ports 50000 and 50001 are exposed so that the database can be accessed from outside of the eianet network. In a production environment this is not necessary, because clients only access Liberty. Check that the container started correctly by using the docker logs: docker logs -f db2 Note: To exit the log display, use Ctrl + C . After you run the Db2 container, set the password for the i2analyze user. Enter the password that you specified in the credentials.properties file for the db2.infostore.password credential. To set the password, run the following command and enter the password when you are prompted: docker exec -u root -it db2 passwd i2analyze Inspect the Dockerfile in the src/images/db2/db2 directory to see the commands that are required to install DB2 on a server in a non-Docker environment. Db2 is installed by using a response file, and an instance of Db2 is created with the required Db2 users. ZooKeeper containers ZooKeeper is the service that is used to maintain configuration information and distributed synchronization across Solr. In this deployment, ZooKeeper is distributed across three servers in a ZooKeeper Quorum. Your configured i2 Analyze toolkit must be installed on each Zookeeper server. To build the zookeeper , zookeeper2 , and zookeeper3 images, run the following commands from the src/images/common folder: docker build -t zookeeper_image zookeeper docker build -t zookeeper2_image zookeeper2 docker build -t zookeeper3_image zookeeper3 Run the ZooKeeper containers: docker run -d --name zookeeper --net eianet --memory=512m -u i2analyze zookeeper_image docker run -d --name zookeeper2 --net eianet --memory=512m -u i2analyze zookeeper2_image docker run -d --name zookeeper3 --net eianet --memory=512m -u i2analyze zookeeper3_image Check that the containers started correctly by using the docker logs: docker logs -f zookeeper docker logs -f zookeeper2 docker logs -f zookeeper3 Inspect the Dockerfile in the src/images/common/zookeeper directory to see the commands that are required to configure a ZooKeeper server in a non-Docker environment. Each ZooKeeper container is started. The container starts and configures ZooKeeper, and hosts the ZooKeeper server. The topology.xml file in the i2 Analyze configuration defines the values for the ZooKeeper server. Admin client container In this deployment, the Admin client is a separate server that is designed for running toolkit tasks in a distributed environment. Your configured i2 Analyze toolkit, and a DB2 client must be installed on the server that you want to use to interact with Liberty and Db2. To build the Admin client image, run the following command from the src/images/db2 folder: docker build -t admin_client_db2_image admin_client The Admin client image is created with the name admin_client_db2_image . Run the Admin client container: docker run -d --name admin_client --net eianet -u i2analyze admin_client_db2_image Inspect the Dockerfile in the src/images/db2/admin_client directory to see the commands that are required to configure the Admin client. Use the following format to run toolkit tasks by using the Admin Client: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t <toolkit task> Solr configuration Before the Solr configuration can be created, all of the ZooKeeper hosts must be running. In the Docker environment, ensure that the ZooKeeper containers are running. You can create and upload the Solr configuration from the Admin client, or you can run the command from one of the ZooKeeper servers. To create the Solr configuration by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createAndUploadSolrConfig --hostname admin_client.eianet The Solr configuration is created and uploaded. Solr containers Solr is used to manage the search index in a deployment of i2 Analyze. In this deployment, Solr is distributed across two servers. Your configured i2 Analyze toolkit must be installed on each Solr server. To build the Solr images, run the following commands from the src/images/common folder: docker build -t solr_image solr docker build -t solr2_image solr2 The Solr images are created with the names solr_image and solr2_image . Run the Solr containers: docker run -d --name solr -p 8983:8983 --net eianet --memory=1g -u i2analyze solr_image docker run -d --name solr2 -p 8984:8984 --net eianet --memory=1g -u i2analyze solr2_image Check that the containers started correctly by using the docker logs: docker logs -f solr docker logs -f solr2 Inspect the Dockerfile in src/images/common/solr and src/images/common/solr2 to see the commands that are required to configure a Solr server in a non-Docker environment. The images include an i2 Analyze topology.xml file that defines the values for the configuration of Solr. When the server starts, the appropriate Solr nodes are started. When you start the Solr container, the port that Solr runs on in the container is specified, in this example it is either 8983 or 8984 . The specified port is mapped on the host machine so that you can access Solr and the Solr web UI from the host machine. After the Solr nodes are running, you can use the Solr Web UI to inspect the Solr and ZooKeeper configurations. Connect to the Solr Web UI on the solr container. In a web browser, go to the following URL to connect to the Solr Web UI: http://localhost:8983/solr/# . The user name is solradmin and the password is the Solr password set in the credentials.properties file. Where the port number is the same as the one that is mapped to local machine when the Solr container is run. The URL uses localhost because the 8983 port is mapped from the host machine to the docker container. In a non-Docker environment, connect by using the host name of the Solr server. Solr collection Before the Solr collection can be created, all of the Solr nodes that the collection comprises must be running. In the Docker environment, ensure that both of the Solr containers are running. You can create the Solr collection from the Admin client, or you can run the command from one of the Solr servers. To create the Solr collection by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createSolrCollections --hostname admin_client.eianet The Solr collection is created. To test the Solr Collection, click Cloud in the Solr Web UI, or you can go to http://localhost:8983/solr/#/~cloud . The user name is solradmin and the password is the Solr password set in the credentials.properties file. A horizontal tree with the collection as the root is displayed. Here you can see the breakdown of the shards, nodes, and replicas on the collection. Create the Information Store database Create the Information Store database in the Db2 instance on the Db2 container. To create the Information Store database by using the Admin client, run the following command: docker exec -t -u i2analyze admin_client bash -c \". home/db2inst1/sqllib/db2profile && /opt/i2/i2analyze/toolkit/scripts/setup -t createDatabases\" To check that the database is created correctly, connect to the database. For example, connect by using IBM Data Studio. The user name is i2analyze and the password is the database password set in the credentials.properties file. Liberty container The Liberty application server hosts the i2 Analyze application, and provides all of the REST services that the client uses. In this deployment, Liberty is deployed on its own server. Your configured i2 Analyze toolkit, and a Db2 client must be installed on the Liberty server. To build the liberty image, run the following command from the src/images/db2 folder: docker build -t liberty_db2_image liberty The Liberty image is created with the name liberty_db2_image . Run the Liberty container: docker run -d --name liberty -p 9082:9082 -p 9445:9445 --net eianet -u i2analyze liberty_db2_image The i2 Analyze application is installed on the Liberty server. Inspect the Dockerfile in the src/images/db2/liberty directory to see the commands that are run to create the Liberty server in a non-Docker environment. The Liberty server is configured, and the opal-server is started. Results After you complete the previous instructions, i2 Analyze is deployed across five Docker containers. By inspecting the Dockerfiles and the toolkit tasks that are used in the previous commands, you can identify the steps that are required to replicate the distributed deployment in a non-Docker environment. Test the deployment To test the deployment, connect to i2 Analyze from Analyst's Notebook Premium. The URL that you use to connect is: http://i2demo:9082/opal . Log in using the user name Jenny with the password Jenny . You can use the Upload records function to add data to the Information Store, and then search for that data."
  },
  "deploy_walk_through_postgres.html": {
    "href": "deploy_walk_through_postgres.html",
    "title": "Deploying the distributed deployment example manually with Postgres",
    "keywords": "Deploying the distributed deployment example manually with Postgres The manual deployment of the distributed deployment example requires you to run each of the steps that were completed automatically in the Quick deploy. The following instructions detail what steps are required on each container, and how to use these steps to deploy i2 Analyze on physical servers. Important : The distributed example uses a Docker environment to demonstrate a distributed deployment of i2 Analyze. The Docker environment is not designed to be used on customer sites for test or production systems. After you understand how the distributed deployment is deployed, replicate the deployment on physical servers. Before you begin Complete the Quick deploy section at least once, before you start the manual deployment. The software prerequisites that you download for the Quick deploy are also used here. Complete the Clean your environment section to ensure that none of the images, containers, or network exist. You must run all Docker commands from a command line where Docker is initialized. Create the network To enable communication between the docker containers, all the containers must be connected to a single network. You can create a bridge network that only the docker containers can access. When each container is run, the --net flag is used to specify that the container connects to a specific network. Create a bridge network called eianet for the distributed deployment example: docker network create eianet Test that the network is created: docker network ls The eianet network is displayed in the list. In a non-Docker environment, ensure that each server that you are using to deploy i2 Analyze can connect to each other. Copy the i2 Analyze configuration Update the distributed deployment example configuration for Postgres. Copy the src/configuration_mods/postgres/base/environment directory to the src/configuration directory. Accept any file overwrites. The i2 Analyze configuration is required by all servers that host components of i2 Analyze, except for the database server. In the src/scripts directory of the distributed deployment example, the copyConfiguration script copies the i2 Analyze configuration from the src/configuration directory to the src/images/<container_name>/configuration directory for each container. From the src/scripts directory, run the copyConfiguration script file. ./copyConfiguration postgres Results In a non-Docker environment, copying the configuration is equivalent to the following steps: Download and extract the i2 Analyze toolkit onto a server. Copy the examples/configurations/information-store-opal/configuration directory into the toolkit directory of your extracted i2 Analyze toolkit. You must populate the configuration for your environment, use the src/configuration directory as a reference. This is the centralized i2 Analyze toolkit and configuration that you copy to your servers. Build the prerequisite images In the distributed deployment example, some of the images are built on top of other images. The prerequisite images must be built first, as described in the following section. Ubuntu toolkit image The ubuntu_toolkit_image is an image that contains the Ubuntu operating system with i2 Analyze installed and contains the i2 Analyze configuration. When you build the ubuntu_toolkit_image , the i2analyze.tar.gz that you copied into the src/images/common/ubuntu_toolkit/i2analyze directory as part of the Quick deploy is copied and extracted into the image. To build the Ubuntu toolkit image, run the following from the src/images/common folder: docker build -t ubuntu_toolkit_image ubuntu_toolkit The Ubuntu toolkit image is created with the name ubuntu_toolkit_image . Configuring and running the containers Each Docker container requires a Docker image. In a non-Docker environment, this is equivalent to configuring and starting a physical server that is used to host a component of i2 Analyze. Postgres container The Postgres container is built from an image that is available from PostGIS on Docker hub, https://hub.docker.com/r/postgis/postgis/ . To build the postgres image, run the following command from the src/images/postgres : docker build -t postgres_image postgres Run the Postgres container: docker run -d -e 'POSTGRES_USER=i2analyze' -e \"POSTGRES_PASSWORD=<PostgresPassword>\" --name postgres -p 5432:5432 --net eianet postgres_image Where <PostgresPassword> is a password for the system administrator user for the Postgres container. The container is started with Postgres installed and running. The port number of the Postgres instance is 5432 . ZooKeeper containers ZooKeeper is the service that is used to maintain configuration information and distributed synchronization across Solr. In this deployment, ZooKeeper is distributed across three servers in a ZooKeeper Quorum. Your configured i2 Analyze toolkit must be installed on each Zookeeper server. To build the zookeeper , zookeeper2 , and zookeeper3 images, run the following commands from the src/images/common folder: docker build -t zookeeper_image zookeeper docker build -t zookeeper2_image zookeeper2 docker build -t zookeeper3_image zookeeper3 Run the ZooKeeper containers: docker run -d --name zookeeper --net eianet --memory=512m -u i2analyze zookeeper_image docker run -d --name zookeeper2 --net eianet --memory=512m -u i2analyze zookeeper2_image docker run -d --name zookeeper3 --net eianet --memory=512m -u i2analyze zookeeper3_image Check that the containers started correctly by using the docker logs: docker logs -f zookeeper docker logs -f zookeeper2 docker logs -f zookeeper3 Inspect the Dockerfile in the src/images/common/zookeeper directory to see the commands that are required to configure a ZooKeeper server in a non-Docker environment. Each ZooKeeper container is started. The container starts and configures ZooKeeper, and hosts the ZooKeeper server. The topology.xml file in the i2 Analyze configuration defines the values for the ZooKeeper server. Admin client container In this deployment, the Admin client is a separate server that is designed for running toolkit tasks in a distributed environment. Your configured i2 Analyze toolkit, and a Postgres client must be installed on the server that you want to use to interact with Liberty and Postgres. To build the Admin client image, run the following command from the src/images/postgres folder: docker build -t admin_client_postgres_image admin_client The Admin client image is created with the name admin_client_postgres_image . Run the Admin client container: docker run -d --name admin_client --net eianet --memory=512m -u i2analyze admin_client_postgres_image Inspect the Dockerfile in the src/images/postgres/admin_client directory to see the commands that are required to configure the Admin client. Use the following format to run toolkit tasks by using the Admin Client: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t <toolkit task> Solr configuration Before the Solr configuration can be created, all of the ZooKeeper hosts must be running. In the Docker environment, ensure that the ZooKeeper containers are running. You can create and upload the Solr configuration from the Admin client, or you can run the command from one of the ZooKeeper servers. To create the Solr configuration by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createAndUploadSolrConfig --hostname admin_client.eianet The Solr configuration is created and uploaded. Solr containers Solr is used to manage the search index in a deployment of i2 Analyze. In this deployment, Solr is distributed across two servers. Your configured i2 Analyze toolkit must be installed on each Solr server. To build the Solr images, run the following commands from the src/images/common folder: docker build -t solr_image solr docker build -t solr2_image solr2 The Solr images are created with the names solr_image and solr2_image . Run the Solr containers: docker run -d --name solr -p 8983:8983 --net eianet --memory=1g -u i2analyze solr_image docker run -d --name solr2 -p 8984:8984 --net eianet --memory=1g -u i2analyze solr2_image Check that the containers started correctly by using the docker logs: docker logs -f solr docker logs -f solr2 Inspect the Dockerfile in src/images/common/solr and src/images/common/solr2 to see the commands that are required to configure a Solr server in a non-Docker environment. The images include an i2 Analyze topology.xml file that defines the values for the configuration of Solr. When the server starts, the appropriate Solr nodes are started. When you start the Solr container, the port that Solr runs on in the container is specified, in this example it is either 8983 or 8984 . The specified port is mapped on the host machine so that you can access Solr and the Solr Web UI from the host machine. After the Solr nodes are running, you can use the Solr Web UI to inspect the Solr and ZooKeeper configurations. Connect to the Solr Web UI on the solr container. In a web browser, go to the following URL to connect to the Solr Web UI: http://localhost:8983/solr/# . The user name is solradmin and the password is the Solr password set in the credentials.properties file. Where the port number is the same as the one that is mapped to local machine when the Solr container is run. The URL uses localhost because the 8983 port is mapped from the host machine to the docker container. In a non-Docker environment, connect by using the host name of the Solr server. Solr collection Before the Solr collection can be created, all of the Solr nodes that the collection comprises must be running. In the Docker environment, ensure that both of the Solr containers are running. You can create the Solr collection from the Admin client, or you can run the command from one of the Solr servers. To create the Solr collection by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createSolrCollections --hostname admin_client.eianet The Solr collection is created. To test the Solr Collection, click Cloud in the Solr Web UI, or you can go to http://localhost:8983/solr/#/~cloud . The user name is solradmin and the password is the Solr password set in the credentials.properties file. A horizontal tree with the collection as the root is displayed. Here you can see the breakdown of the shards, nodes, and replicas on the collection. Create the Information Store database Create the Information Store database in the Postgres instance on the Postgres container. To create the Information Store database by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createDatabases To check that the database is created correctly, connect to the database. For example, connect by using pgAdmin. The user name is i2analyze and the password is the database password set in the credentials.properties file. Liberty container The Liberty application server hosts the i2 Analyze application, and provides all of the REST services that the client uses. In this deployment, Liberty is deployed on its own server. Your configured i2 Analyze toolkit, and the Postgres client tools must be installed on the Liberty server. To build the liberty image, run the following command from the src/images/postgres folder: docker build -t liberty_postgres_image liberty The Liberty image is created with the name liberty_postgres_image . Run the Liberty container: docker run -d --name liberty -p 9082:9082 -p 9445:9445 --net eianet --memory=2g -u i2analyze liberty_postgres_image The i2 Analyze application is installed on the Liberty server. Inspect the Dockerfile in the src/images/postgres/liberty directory to see the commands that are run to create the Liberty server in a non-Docker environment. The Liberty server is configured, and the opal-server is started. Results After you complete the previous instructions, i2 Analyze is deployed across five Docker containers. By inspecting the Dockerfiles and the toolkit tasks that are used in the previous commands, you can identify the steps that are required to replicate the distributed deployment in a non-Docker environment. Test the deployment To test the deployment, connect to i2 Analyze from Analyst's Notebook Premium. The URL that you use to connect is: http://i2demo:9082/opal . Log in using the user name Jenny with the password Jenny . You can use the Upload records function to add data to the Information Store, and then search for that data."
  },
  "deploy_walk_through_sqlserver.html": {
    "href": "deploy_walk_through_sqlserver.html",
    "title": "Deploying the distributed deployment example manually with SQL Server",
    "keywords": "Deploying the distributed deployment example manually with SQL Server The manual deployment of the distributed deployment example requires you to run each of the steps that were completed automatically in the Quick deploy. The following instructions detail what steps are required on each container, and how to use these steps to deploy i2 Analyze on physical servers. Important : The distributed example uses a Docker environment to demonstrate a distributed deployment of i2 Analyze. The Docker environment is not designed to be used on customer sites for test or production systems. After you understand how the distributed deployment is deployed, replicate the deployment on physical servers. Before you begin Complete the Quick deploy section at least once, before you start the manual deployment. The software prerequisites that you download for the Quick deploy are also used here. Complete the Clean your environment section to ensure that none of the images, containers, or network exist. You must run all Docker commands from a command line where Docker is initialized. Create the network To enable communication between the docker containers, all the containers must be connected to a single network. You can create a bridge network that only the docker containers can access. When each container is run, the --net flag is used to specify that the container connects to a specific network. Create a bridge network called eianet for the distributed deployment example: docker network create eianet Test that the network is created: docker network ls The eianet network is displayed in the list. In a non-Docker environment, ensure that each server that you are using to deploy i2 Analyze can connect to each other. Copy the i2 Analyze configuration The i2 Analyze configuration is required by all servers that host components of i2 Analyze, except for the database server. In the src/scripts directory of the distributed deployment example, the copyConfiguration script copies the i2 Analyze configuration from the src/configuration directory to the <container_name>/configuration directory for each container. From the src/scripts directory, run the copyConfiguration script file. ./copyConfiguration Results In a non-Docker environment, copying the configuration is equivalent to the following steps: Download and extract the i2 Analyze toolkit onto a server. Copy the examples/configurations/information-store-opal/configuration directory into the toolkit directory of your extracted i2 Analyze toolkit. You must populate the configuration for your environment, use the src/configuration directory as a reference. This is the centralized i2 Analyze toolkit and configuration that you copy to your servers. Build the prerequisite images In the distributed deployment example, some of the images are built on top of other images. The prerequisite images must be built first, as described in the following section. Ubuntu toolkit image The ubuntu_toolkit_image is an image that contains the Ubuntu operating system with i2 Analyze installed and contains the i2 Analyze configuration. When you build the ubuntu_toolkit_image , the i2analyze.tar.gz that you copied into the src/images/common/ubuntu_toolkit/i2analyze directory as part of the Quick deploy is copied and extracted into the image. To build the Ubuntu toolkit image, run the following from the src/images/common folder: docker build -t ubuntu_toolkit_image ubuntu_toolkit The Ubuntu toolkit image is created with the name ubuntu_toolkit_image . Configuring and running the containers Each Docker container requires a Docker image. In a non-Docker environment, this is equivalent to configuring and starting a physical server that is used to host a component of i2 Analyze. SQL Server container The SQL Server container is built from an image that is available from Microsoft on Docker hub, https://hub.docker.com/_/microsoft-mssql-server . To build the sqlserver image, run the following command from the src/images/sqlserver : docker build -t sqlserver_image sqlserver Run the SQL Server container: docker run -d -e 'ACCEPT_EULA=Y' -e \"SA_PASSWORD=<SAPassword>\" --name sqlserver -p 1433:1433 --net eianet sqlserver_image Where <SAPassword> is a password for the system administrator user for the SQL Server container. Run the following command to add the i2 Analyze user with the required permissions: docker exec -t sqlserver /opt/mssql-tools/bin/sqlcmd -U SA -P <SAPassword> -Q \"CREATE LOGIN i2analyze WITH PASSWORD = '<Password>'; CREATE USER i2analyze FOR LOGIN i2analyze; ALTER SERVER ROLE sysadmin ADD MEMBER i2analyze\" Where <SAPassword> is the same password that you entered in the previous command, and <Password> is the password that you specified in the credentials.properties file for the db.infostore.password credential. The container is started with SQL Server installed and running. The port number of the SQL Server instance is 1433 . ZooKeeper containers ZooKeeper is the service that is used to maintain configuration information and distributed synchronization across Solr. In this deployment, ZooKeeper is distributed across three servers in a ZooKeeper Quorum. Your configured i2 Analyze toolkit must be installed on each Zookeeper server. To build the zookeeper , zookeeper2 , and zookeeper3 images, run the following commands from the src/images/common folder: docker build -t zookeeper_image zookeeper docker build -t zookeeper2_image zookeeper2 docker build -t zookeeper3_image zookeeper3 Run the ZooKeeper containers: docker run -d --name zookeeper --net eianet --memory=512m -u i2analyze zookeeper_image docker run -d --name zookeeper2 --net eianet --memory=512m -u i2analyze zookeeper2_image docker run -d --name zookeeper3 --net eianet --memory=512m -u i2analyze zookeeper3_image Check that the containers started correctly by using the docker logs: docker logs -f zookeeper docker logs -f zookeeper2 docker logs -f zookeeper3 Inspect the Dockerfile in the src/images/common/zookeeper directory to see the commands that are required to configure a ZooKeeper server in a non-Docker environment. Each ZooKeeper container is started. The container starts and configures ZooKeeper, and hosts the ZooKeeper server. The topology.xml file in the i2 Analyze configuration defines the values for the ZooKeeper server. Admin client container In this deployment, the Admin client is a separate server that is designed for running toolkit tasks in a distributed environment. Your configured i2 Analyze toolkit, and a SQL Server client must be installed on the server that you want to use to interact with Liberty and SQL Server. To build the Admin client image, run the following command from the src/images/sqlserver folder: docker build -t admin_client_sqlserver_image admin_client The Admin client image is created with the name admin_client_sqlserver_image . Run the Admin client container: docker run -d --name admin_client --net eianet --memory=512m -u i2analyze admin_client_sqlserver_image Inspect the Dockerfile in the src/images/sqlserver/admin_client directory to see the commands that are required to configure the Admin client. Use the following format to run toolkit tasks by using the Admin Client: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t <toolkit task> Solr configuration Before the Solr configuration can be created, all of the ZooKeeper hosts must be running. In the Docker environment, ensure that the ZooKeeper containers are running. You can create and upload the Solr configuration from the Admin client, or you can run the command from one of the ZooKeeper servers. To create the Solr configuration by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createAndUploadSolrConfig --hostname admin_client.eianet The Solr configuration is created and uploaded. Solr containers Solr is used to manage the search index in a deployment of i2 Analyze. In this deployment, Solr is distributed across two servers. Your configured i2 Analyze toolkit must be installed on each Solr server. To build the Solr images, run the following commands from the src/images/common folder: docker build -t solr_image solr docker build -t solr2_image solr2 The Solr images are created with the names solr_image and solr2_image . Run the Solr containers: docker run -d --name solr -p 8983:8983 --net eianet --memory=1g -u i2analyze solr_image docker run -d --name solr2 -p 8984:8984 --net eianet --memory=1g -u i2analyze solr2_image Check that the containers started correctly by using the docker logs: docker logs -f solr docker logs -f solr2 Inspect the Dockerfile in src/images/common/solr and src/images/common/solr2 to see the commands that are required to configure a Solr server in a non-Docker environment. The images include an i2 Analyze topology.xml file that defines the values for the configuration of Solr. When the server starts, the appropriate Solr nodes are started. When you start the Solr container, the port that Solr runs on in the container is specified, in this example it is either 8983 or 8984 . The specified port is mapped on the host machine so that you can access Solr and the Solr Web UI from the host machine. After the Solr nodes are running, you can use the Solr Web UI to inspect the Solr and ZooKeeper configurations. Connect to the Solr Web UI on the solr container. In a web browser, go to the following URL to connect to the Solr Web UI: http://localhost:8983/solr/# . The user name is solradmin and the password is the Solr password set in the credentials.properties file. Where the port number is the same as the one that is mapped to local machine when the Solr container is run. The URL uses localhost because the 8983 port is mapped from the host machine to the docker container. In a non-Docker environment, connect by using the host name of the Solr server. Solr collection Before the Solr collection can be created, all of the Solr nodes that the collection comprises must be running. In the Docker environment, ensure that both of the Solr containers are running. You can create the Solr collection from the Admin client, or you can run the command from one of the Solr servers. To create the Solr collection by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createSolrCollections --hostname admin_client.eianet The Solr collection is created. To test the Solr Collection, click Cloud in the Solr Web UI, or you can go to http://localhost:8983/solr/#/~cloud . The user name is solradmin and the password is the Solr password set in the credentials.properties file. A horizontal tree with the collection as the root is displayed. Here you can see the breakdown of the shards, nodes, and replicas on the collection. Create the Information Store database Create the Information Store database in the SQL Server instance on the SQL Server container. To create the Information Store database by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createDatabases To check that the database is created correctly, connect to the database. For example, connect by using SQL Server Management Studio. The user name is i2analyze and the password is the database password set in the credentials.properties file. Liberty container The Liberty application server hosts the i2 Analyze application, and provides all of the REST services that the client uses. In this deployment, Liberty is deployed on its own server. Your configured i2 Analyze toolkit, and the SQL Server client tools must be installed on the Liberty server. To build the liberty image, run the following command from the src/images/sqlserver folder: docker build -t liberty_sqlserver_image liberty The Liberty image is created with the name liberty_sqlserver_image . Run the Liberty container: docker run -d --name liberty -p 9082:9082 -p 9445:9445 --net eianet --memory=2g -u i2analyze liberty_sqlserver_image The i2 Analyze application is installed on the Liberty server. Inspect the Dockerfile in the src/images/sqlserver/liberty directory to see the commands that are run to create the Liberty server in a non-Docker environment. The Liberty server is configured, and the opal-server is started. Results After you complete the previous instructions, i2 Analyze is deployed across five Docker containers. By inspecting the Dockerfiles and the toolkit tasks that are used in the previous commands, you can identify the steps that are required to replicate the distributed deployment in a non-Docker environment. Test the deployment To test the deployment, connect to i2 Analyze from Analyst's Notebook Premium. The URL that you use to connect is: http://i2demo:9082/opal . Log in using the user name Jenny with the password Jenny . You can use the Upload records function to add data to the Information Store, and then search for that data."
  },
  "index.html": {
    "href": "index.html",
    "title": "Distributed deployment of i2 Analyze example",
    "keywords": "Distributed deployment of i2 Analyze example An example environment is provided to demonstrate how to build a distributed deployment of i2 Analyze on multiple servers. The following documentation is provided so that you can understand, deploy, and configure the distributed deployment example. Important : The distributed deployment example uses a Docker environment to demonstrate a distributed deployment of i2 Analyze. The Docker environment is not designed to be used on customer sites for test or production systems. The objective of the distributed deployment example on Docker is to give you an understanding of how a distributed deployment of i2 Analyze is deployed, so that you can replicate the deployment on physical servers. This information is intended for users who want to learn how to deploy i2 Analyze distributed across multiple servers. Readers must be familiar with i2 Analyze and the prerequisite components. The documentation for understanding, deploying, and configuring the distributed deployment example is divided into the following sections: Understand the distributed deployment example: Understanding the example Understanding Docker in the distributed deployment example Deploy the distributed deployment example: Quick start Deploying the distributed deployment example manually for SQL Server Deploying the distributed deployment example manually for Db2 Deploying the distributed deployment example manually for Postgres Change the database management system of your example Keystores and certificates for components of i2 Analyze Deploying the distributed deployment example with security Deploying i2 Analyze with i2 Connect Upgrading a distributed deployment to version 4.4.3 of i2 Analyze Cleaning your environment Configure the deployment: Updating the schema Updating the security schema Changing the search results filtering Changing Visual Query conditions Clearing data from the system Resetting the system Configuring Liberty Ingesting data remotely Choosing the method for configuring Solr Creating a Solr collection using the createSolrCollection script Adding Solr replicas to your deployment Adding Solr shards to your deployment Adding another Solr server and Solr node Changing the memory allocation for a Solr node"
  },
  "securing_certificates.html": {
    "href": "securing_certificates.html",
    "title": "Keystores and certificates for components of i2 Analyze",
    "keywords": "Keystores and certificates for components of i2 Analyze SSL communication relies on encryption, keys, and certificates to initiate a secure connection. The certificates are stored in keystore files on the client and the server. Certificates are exchanged to establish trust during the handshake process that initiates a secure connection. When a certificate is granted through a certificate authority, that certificate can be trusted by the clients or applications that trust certificates that are signed by that authority. A public key certificate that authenticates a server is stored in a keystore file on the server. Trusted certificate authority certificates are stored in the client's truststore file. As part of the SSL handshake process, certificates are exchanged that are signed by a trusted certificate authority to verify that a certificate is authentic. For more information about the keystores and certificates that are required in an i2 Analyze deployment, see SSL certificates for i2 Analyze and SSL keystores for i2 Analyze . As part of the distributed deployment example, scripts are provided that demonstrate how to create the required keystores and certificates. The certificates are created and signed by a certificate authority, which is also included as part of the example. To create the keystores, certificates, and certificate authority, run the createKeysAndStores script file. These scripts contain comments that explain the processes that are completed. As an overview, a certificate authority image is built and run as a certificate authority container. A keystore, truststore, and private key certificate are created for each component of i2 Analyze. A certificate signing request is created for each certificate. Each signing request is sent to the certificate authority, which signs and returns the certificate. At the end of this process, each container contains a keystore that is populated with its signed private key certificate and a truststore that is populated with the certificate authority's signer certificate. The certificate authority's signer certificate is used to verify the certificates that are received from the other components of i2 Analyze. After you run the createKeysAndStores script, the environment is as follows: In a non-Docker environment, you must ensure that the correct stores and certificates are in place. You can use the example scripts and information in the i2 documentation to identify methods for creating the stores."
  },
  "securing_ssl.html": {
    "href": "securing_ssl.html",
    "title": "Deploying the distributed deployment example with SSL",
    "keywords": "Deploying the distributed deployment example with SSL You can deploy the distributed deployment example with additional security configured. You can use the scripts and configurations that are provided to understand how to deploy with security in a non-Docker environment. Before you begin You must complete the Quick deploy or deploy the example manually, and your deployment must be running. If you deployed the example with one of the configurations in the configuration_mods directory, except the db2/base or sqlserver/base ones, or changed the topology.xml file. Reset your example deployment to the base configuration before you deploy with security. To reset your environment, run the following command from the src/scripts directory: ./resetEnvironment Choosing a configuration There are a number of secure configurations that you can choose to deploy: ssl This configuration deploys with SSL configured between each of the distributed components in an i2 Analyze deployment. client_cert_ssl This configuration deploys with SSL configured between each of the distributed components in an i2 Analyze deployment. The user can log in to the system by using a client certificate. For more information about configuring client certificate authentication, see Configuring X.509 client certificate authentication with i2 Analyze Creating the keystores and certificates Run the createKeysAndStores script to create the stores, certificates, and certificate authority. For example, run the following command from the src/scripts directory: ./createKeysAndStores You are prompted to enter a password that is used for each of the keystores and truststores that are created. The password that you specify here is used later. For more information about the stores and certificates that are created, see Keystores and certificates for components of i2 Analyze . Specifying the credentials You must specify the credentials for your deployment in the src/configuration/environment/credentials.properties . Set the passwords to use for each of the keystore and truststore credentials. The passwords must match the value that you entered when you ran the createKeysAndStores script. For more information about the credentials file, see Modifying the credentials . Updating your deployment to use a secure configuration Run the setupSecurity script, and pass the name of the configuration as a parameter. For example, to update the deployment with the ssl configuration, run the following command from the src/scripts directory: ./setupSecurity ssl Results The deployment of i2 Analyze is updated with your chosen configuration. Any connections that are secured by using SSL use the certificates and stores that are present on each server. Connecting to Analyst's Notebook Premium To connect to i2 Analyze in a deployment that uses SSL, you must use a different URL. The URL that you use to connect is: https://i2demo:9445/opal . The protocol is changed to https , and the port number is changed to 9445 . Learning how to deploy the configuration in a non-Docker environment To understand the differences between the configurations, and the base configuration, you can use a file comparison tool. You can see which properties files are modified, and the configuration settings that are required. For detailed explanations of the properties and the process for changing them, you can use the information that is provided in the i2 documentation. For information about securing the components of i2 Analyze, see Secure Sockets Layer connections with i2 Analyze and Configuring X.509 client certificate authentication with i2 Analyze . To see the toolkit commands that are run to deploy the components of i2 Analyze in a secure deployment, you can inspect the setupSecurity script. The instructions in the i2 documentation use self-signed certificates to demonstrate securing a deployment. For more information about using certificates that are signed by certificate authority, see Keystores and certificates for components of i2 Analyze ."
  },
  "understand_example.html": {
    "href": "understand_example.html",
    "title": "Understanding the distributed deployment example",
    "keywords": "Understanding the distributed deployment example The distributed deployment example demonstrates how to build a distributed deployment of i2 Analyze on multiple servers. To avoid the need for multiple physical servers, Docker is used to simulate the servers. The distributed deployment example not only provides the environment for the deployment, but configures and deploys i2 Analyze across the servers in the same way that a system implementer does. In the distributed deployment example, i2 Analyze with the Opal services is deployed on seven servers: One for Liberty and the i2 Analyze application One for the database management system for the Information Store database Two for each Solr node Three for ZooKeeper that manages the Solr configuration The following diagram shows the servers that i2 Analyze is deployed on, and what is required on each server: In addition to the containers that replicate physical servers in a deployment of i2 Analyze, as part of the distributed deployment example, extra containers are used to configure the Docker environment. These extra containers do not represent physical servers in a non-Docker environment. The additional containers are: Ubuntu toolkit In the example Docker environment, i2 Analyze is installed on the Ubuntu operating system. Each container with i2 Analyze installed is based on this container. Note: Installation of i2 Analyze on Ubuntu is not supported in any environment other than the distributed deployment example. Db2 installer This container contains the prerequisites for Db2 to be installed in the Docker environment. It is used by the Db2 container to install Db2. Base client This container is used to install the Db2 Client. It is used by the Liberty and Admin client containers to install the Db2 Client when you are using Db2 as the database management system. Admin client There are three versions of the admin client container in the example, one for SQL Server, one for Db2 and one for Postgres. This container contains an installation of i2 Analyze and the client required to connect to the remote database. In the distributed deployment example, this container is used to run commands that impact other containers. In a non-Docker environment, you can run these commands on the specific physical server. Read the following documentation and Dockerfiles to gain an understanding of how to deploy a similar i2 Analyze system in a non-Docker environment, on physical servers. All of the steps that you must complete to deploy a distributed i2 Analyze system are demonstrated in the distributed deployment example. After you deploy the distributed deployment example, information is provided for how to complete configuration tasks in a distributed deployment of i2 Analyze. Host names in a distributed environment When i2 Analyze is deployed in a distributed environment, the topology.xml file contains multiple host names. In the distributed deployment example, the host names that are used include; sqlserver , db2 , postgres , solr , solr2 , zookeeper , zookeeper2 , zookeeper3 , and liberty . To deploy and configure i2 Analyze is a distributed environment, you can use the --hostname argument restrict the effect of the toolkit task to the host name that you provide. For example, setup -t startSolrNodes --hostname solr.eianet starts the Solr nodes on the server with the host name solr . If you run setup -t startSolr --hostname solr2.eianet , only the Solr nodes on the server with host name solr2 start. If a toolkit task that you run with a specific host name impacts multiple servers, you must run that toolkit task for each server with their host name specified. Configurations The distributed deployment example includes a number of configurations that can be used to demonstrate a deployment of i2 Analyze. The configuration in the src/configuration directory is the default configuration for the distributed deployment example. i2 Analyze is deployed in the topology that is described above, with the Information Store on SQL Server. The src/configuration_mods directory contains the configuration modifications, separated into database management system, that are required to configure i2 Analyze for specific deployment patterns or features. You can compare the changes in these directories to the src/configuration directory to identify what is required for these deployments. SSL deployments The configurations that include ssl secure connection to and between components of i2 Analyze. For more information, see Deploying the distributed deployment example with security . i2 Connect The i2connect configuration deploys i2 Analyze with i2 Connect and an example connector. For more information about configuring client certificate authentication, see Deploying the distributed deployment example with i2 Connect ."
  },
  "understand_example_docker.html": {
    "href": "understand_example_docker.html",
    "title": "Understanding Docker in the distributed deployment example",
    "keywords": "Understanding Docker in the distributed deployment example The distributed deployment example demonstrates how to build a distributed deployment of i2 Analyze on multiple servers. To avoid the need for multiple physical servers, Docker is used to simulate the servers. Docker Docker is used to build and maintain Docker images and containers. For more information about Docker, see Docker overview . Docker containers and images A Docker container represents a physical server in a non-Docker environment. In the distributed deployment example i2 Analyze is deployed across the Docker containers, this represents the deployment of i2 Analyze across physical servers in a non-Docker environment. In the example, there are Docker images and containers for the servers that host the components of i2 Analyze. Additionally, the Ubuntu toolkit image has the files that make up Ubuntu with the i2 Analyze toolkit installed and the base configuration image contains the i2 Analyze configuration files that are not modified as part of this example. In a non-Docker environment, this might be a server that you are not using as part of the deployment. To simulate starting a server, run a container from its corresponding image. For more information about Docker containers and images, see Docker objects . Dockerfiles In the distributed deployment example, Dockerfiles are used to define what is installed onto the file system and what is run on startup of the server. For more information about Dockerfile definitions, see Dockerfile format . The Dockerfiles are in each src/images/<container_name> directory. You can use a text editor to inspect the Dockerfiles . The Dockerfiles contain the commands that are used to build the Docker image. Some commands in the Dockerfiles are specific to Docker, and required for the distributed deployment example to work in a Docker environment. However, all of the actions or commands that a system implementer must complete in a non-Docker environment are surfaced here. To identify the commands a system implementer requires, you must understand the Dockerfiles . The following instructions are used in commands in the Dockerfiles : The COPY instruction copies files or directories to the file system of the container. To replicate the COPY instruction in a non-Docker environment, copy the files or directories to same location on the physical server that the container represents. The RUN instruction runs commands that add files into the image. It is used to run installers, toolkit commands, and other commands that add files into the image. To replicate the RUN instruction in a non-Docker environment, run the same commands in the same order on the physical server that the container represents. The ENTRYPOINT instruction represents commands that are run at server startup. To replicate the ENTRYPOINT instruction in a non-Docker environment, run the same command on the physical server that the container represents. The ENTRYPOINT also has a command that never completes so that the Docker container continues to run. For example, && tail -f /dev/null . This applies to the Docker environment only. The EXPOSE instruction lists the ports that the container listens on. This is used so that the host machine can access these ports on the Docker network that the containers use. The ENV instruction sets the host name of the Docker container. The host names are used in the topology.xml . Extra instructions in the Dockerfiles : The FROM and MAINTAINER instructions complete Docker specific configuration, and do not need to be replicated in a non-Docker environment."
  },
  "upgrade_walk_through.html": {
    "href": "upgrade_walk_through.html",
    "title": "Upgrading a distributed deployment to version 4.4.3 of i2 Analyze",
    "keywords": "Upgrading a distributed deployment to version 4.4.3 of i2 Analyze To upgrade the distributed deployment example, you must run an upgrade command on each server in your deployment. The following instructions detail what is required on each container, and how to use these steps to upgrade i2 Analyze on physical servers. Before you begin You must have a working distributed deployment of a previous version of i2 Analyze, completed with a previous version of the distributed deployment example. Important If you are upgrading a distributed deployment that was created by using V2.6.x or earlier of the distributed deployment example, you must use the /opt/IBM/i2analyze path instead of /opt/i2/i2analyze in the Docker commands on this page. If you are upgrading a distributed deployment that was created by using V1.x of the distributed deployment example, you must not update your distributed deployment example source code. Continue to use the same scripts, images, and containers from V1.x of the distributed deployment example. You must run any commands on the Docker container as the root user instead of the i2analyze user. For example, docker exec -u root liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty instead of docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty . Prerequisites for upgrading the distributed deployment Download, install, and configure the prerequisites for upgrading a distributed deployment. Example code Clone or download the distributed deployment example from https://github.com/i2group/analyze-deployment/releases . Remove the current distributed deployment example code, but ensure the containers stay running. Extract the distributed deployment example that you just downloaded. i2 Analyze Download i2 Analyze for Linux. You download the I2A_V4.4.3_LINUX.tar.gz version 4.4.3 by using the usual mechanism. Rename the tar.gz file to i2analyze.tar.gz . Copy the archive file to the src/images/common/ubuntu_toolkit/i2analyze directory. Add the mssql-jdbc-7.4.1.jre11.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: Download the Microsoft JDBC Driver 7.4 for SQL Server from Microsoft JDBC Driver 7.4 for SQL Server . Extract the contents of the tar.gz , and locate the sqljdbc_7.4\\enu\\mssql-jdbc-7.4.1.jre11.jar file. Add the postgresql-xx.x.x.jar file to the src/configuration/environment/common/jdbc-drivers directory. Note: Download the latest (Java 8) PostgreSQL JDBC Driver for PostgreSQL from https://jdbc.postgresql.org/download/ Analyst's Notebook Premium Download i2 Analyst's Notebook Premium version 10 by using the usual mechanism. Upgrade Analyst's Notebook Premium. For more information, see Upgrading i2 Analyst's Notebook Premium . Preparing i2 Analyze Back up and install i2 Analyze before you upgrade i2 Analyze. Stopping Liberty Before you upgrade your deployment, you must stop the application server. To stop Liberty, run the following command on the liberty container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t stopLiberty The console output from the stopLiberty task is output directly to the console. Backing up the configuration Before you upgrade your deployment, you must backup your current configuration. In the distributed deployment example, the backupConfiguration script creates a copy of your configuration directory on the admin_client container. The backup of your configuration is in the src/configuration_mods/configuration_backup directory. In a non-Docker environment, ensure that you have a copy of the configuration directory that contains your complete configuration in a location outside of the i2 Analyze toolkit. The process of upgrading i2 Analyze removes the deployment toolkit from each server in the deployment. Ensure that the backup of your configuration is up-to-date for each server in your deployment. Installing i2 Analyze You must install version 4.4.3 of i2 Analyze on each server in the deployment. In the distributed deployment example, you can use the installToolkit script to install i2 Analyze on each container. To install i2 Analyze on each container, run the following command from the src/scripts directory: ./installToolkit ./images/common/ubuntu_toolkit/i2analyze/i2analyze.tar.gz In a non-Docker environment, install i2 Analyze on each server by using the following instructions: Upgrading i2 Analyze . Copying the backed up configuration In the distributed deployment example, you upgrade the configuration on the admin_client container. You must copy the backed up configuration of the deployment toolkit to the Admin Client. To copy your backed up configuration to the admin_client container, run the following command from the src directory: docker cp ./configuration_mods/configuration_backup admin_client:/opt/i2/i2analyze/toolkit/configuration When the directory is copied, the owner of the directory, and all files within it, is changed. The user that runs the deployment script must be able to write to files within the configuration directory. To change the ownership of the directory and the files, run the following command: docker exec -u root admin_client chown -R i2analyze:i2analyze /opt/i2/i2analyze/toolkit/configuration The configuration is now on the admin_client container. In a non-Docker environment, copy your backed up configuration directory to the i2analyze/toolkit/configuration directory on your equivalent of the Admin Client server. You can complete a quick upgrade or you can upgrade manually. Completing the upgrade manually requires you to run each of the steps completed automatically in the quick upgrade. By upgrading manually, you can see in detail what is required on each container and how to use this to upgrade a deployment on physical servers. Quick upgrade To upgrade the distributed deployment example, a script is provided that upgrades i2 Analyze and its components. To upgrade your deployment, run the ./upgradeDeployment script from the src/scripts directory. All of the components of i2 Analyze are upgraded and the application is started. You can test the deployment by completing the steps in Testing the deployment . Upgrade the deployment manually Upgrading and copying the configuration If you are using a newer version of JDBC Drivers, remove the previous version, run the following commands from the src directory: docker exec -u root admin_client rm -rf /opt/i2/i2analyze/toolkit/configuration/environment/common/jdbc-drivers docker cp configuration/environment/common/jdbc-drivers admin_client:/opt/i2/i2analyze/toolkit/configuration/environment/common Upgrade the i2 Analyze configuration to version 4.4.3. To upgrade the configuration, run the following command on the admin_client container: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeConfiguration The console output from the upgradeConfiguration task is output directly to the console. The upgraded configuration must be copied to each of the servers in the deployment. In a Docker environment, first you must copy the upgraded configuration to your local machine by running the following command inside the src/scripts directory: For SQL Server: docker cp admin_client:/opt/i2/i2analyze/toolkit/configuration ../configuration_mods/sqlserver/configuration_upgraded For Db2: docker cp admin_client:/opt/i2/i2analyze/toolkit/configuration ../configuration_mods/db2/configuration_upgraded For Postgres: docker cp admin_client:/opt/i2/i2analyze/toolkit/configuration ../configuration_mods/postgres/configuration_upgraded Then, copy the upgraded configuration to each server. In the distributed deployment example, you can run the updateServerConfigurations script to copy the configuration to each container: ./updateServerConfigurations configuration_upgraded In a non-Docker environment, copy the upgraded configuration to the i2analyze/toolkit/configuration directory on each server. Upgrading and starting the components of i2 Analyze After the upgraded configuration is present on each server, you must upgrade each component of the deployment to version 4.4.3. You must upgrade and start the components of a deployment in the order that they are described here. Upgrading ZooKeeper To upgrade ZooKeeper, run the following command on the zookeeper , zookeeper2 and zookeeper3 containers: docker exec -u i2analyze zookeeper /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeZookeeper --hostname zookeeper.eianet docker exec -u i2analyze zookeeper2 /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeZookeeper --hostname zookeeper2.eianet docker exec -u i2analyze zookeeper3 /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeZookeeper --hostname zookeeper3.eianet The console output from the upgradeZookeeper task is output directly to the console. Upgrading Solr To upgrade Solr, run the following command on the solr and solr2 containers: docker exec -u i2analyze solr /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeSolr --hostname solr.eianet docker exec -u i2analyze solr2 /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeSolr --hostname solr2.eianet The console output from the upgradeSolr task is output directly to the console. Upgrading the Information Store database To upgrade the Information Store database, run the following command on the admin_client container: docker exec -u i2analyze admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeDatabases --hostname admin_client.eianet The console output from the upgradeDatabases task is output directly to the console. Starting ZooKeeper To start ZooKeeper, run the following commands on the zookeeper , zookeeper2 and zookeeper3 containers: docker exec -u i2analyze zookeeper /opt/i2/i2analyze/toolkit/scripts/setup -t startZkHosts --hostname zookeeper.eianet docker exec -u i2analyze zookeeper2 /opt/i2/i2analyze/toolkit/scripts/setup -t startZkHosts --hostname zookeeper2.eianet docker exec -u i2analyze zookeeper3 /opt/i2/i2analyze/toolkit/scripts/setup -t startZkHosts --hostname zookeeper3.eianet The console output from the startZkHosts task is output directly to the console. Upload the Solr configuration on ZooKeeper To upload the Solr configuration by using the Admin client, run the following command: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t createAndUploadSolrConfig --hostname admin_client.eianet The Solr configuration is uploaded. Starting Solr To start Solr, run the following commands on the solr and solr2 containers: docker exec -u i2analyze solr /opt/i2/i2analyze/toolkit/scripts/setup -t startSolr --hostname solr.eianet docker exec -u i2analyze solr2 /opt/i2/i2analyze/toolkit/scripts/setup -t startSolr --hostname solr2.eianet The console output from the startSolr tasks are displayed directly to the console. Upgrading the Solr collection To upgrade the Solr collection, run the following command on the admin_client container: docker exec -u i2analyze -t admin_client /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeSolrCollections --hostname admin_client.eianet The console output from the upgradeSolrCollections task is output directly to the console. Upgrading Liberty To upgrade Liberty and the i2 Analyze application, run the following command on the liberty container: docker exec -u i2analyze -t liberty /opt/i2/i2analyze/toolkit/scripts/setup -t upgradeLiberty The console output from the upgradeLiberty task is output directly to the console. Starting Liberty Start Liberty with the upgraded i2 Analyze application. To start Liberty, run the following command on the liberty container: docker exec -u i2analyze liberty /opt/i2/i2analyze/toolkit/scripts/setup -t startLiberty The console output from the startLiberty task is output directly to the console. (Optional) Upgrading the ETL Client If your deployment uses an ETL Client, you must recreate it with the new upgraded toolkit. In the distributed deployment example, the deployEtlClient script removes the old installation from your running etl_client container, creates a new etl_client in the admin_client container and copies it over to the etl_client container creates a copy of your configuration directory on the Admin Client container. The backup of your configuration is in the src/configuration_mods/configuration_backup directory. Results All of the components of i2 Analyze are upgraded and the application is started. Testing the deployment To test the deployment upgraded successfully, connect to i2 Analyze from Analyst's Notebook Premium. The URL that you use to connect is the same as you used to before you run the upgrade process. Log in using the user name Jenny with the password Jenny . You can use the Upload records functionality to add data to the Information Store, and then search for that data or data that already existed."
  }
}